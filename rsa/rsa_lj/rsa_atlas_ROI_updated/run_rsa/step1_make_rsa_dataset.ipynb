{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b5a2bc5",
   "metadata": {},
   "source": [
    "# RSA analysis for 2023 fMRI data\n",
    "### By: Linjing Jiang\n",
    "### Updated on 5/25/2024\n",
    "\n",
    "This script implements an RSA analysis for fMRI subjects collected from July 2023 to Dec 2024.\n",
    "Here, I used the most updated RSA toolbox: https://rsatoolbox.readthedocs.io/en/stable/getting_started.html, \n",
    "https://github.com/rsagroup/rsatoolbox.\n",
    "\n",
    "Before running this analysis, please run the following in order:\n",
    "1. Spatiotemporal GLM first level in SPM (MATLAB) using '/mnt/Data1/linjdata1/vswmda/script/spm_glm/first_level_glm_spatiotemporal.m'. The output is under '/mnt/Data1/linjdata1/vswmda/scan_data/spm/output/preproc_st_spatiotemporal_censor'\n",
    "\n",
    "2. Extract Beta map per experimental condition per run per ROI,using the '/mnt/Data1/linjdata1/vswmda/script/rsa_lj/prep_beta_map_for_mvpa.m' and '/mnt/Data1/linjdata1/vswmda/script/rsa_lj/get_beta_labels.m'. The output is under '/mnt/Data1/linjdata1/vswmda/scan_data/rsa/beta_run_rsa/f16/beta'\n",
    "\n",
    "3. Prepare beta maps for rsa, using '/mnt/Data1/linjdata1/vswmda/script/rsa_lj/rsa_data_prep_beta_run_2024.m'. The output is a variable called 'responsePatterns' that is stored under '/mnt/Data1/linjdata1/vswmda/scan_data/rsa/beta_run_rsa/f16/'\n",
    "\n",
    "After running these scripts, you will get a .mat files under the '/mnt/Data1/linjdata1/vswmda/scan_data/rsa/beta_run_rsa/f16/' folder:\n",
    "\n",
    "1) responsePattern.mat: Contains extracted bold data across voxels for different ROIs and different sessions/participants.\n",
    "- 'responsePatterns': data ready for RSA\n",
    "- This is a 1 x 1 structure, containing \n",
    "    - responsePatterns.stim\n",
    "    - responsePatterns.delay\n",
    "    - responsePatterns.response\n",
    "    - corresponding to different task epochs(stimulus, delay, and response)\n",
    "    - output directory:  '/mnt/Data1/linjdata1/vswmda/scan_data/rsa/beta_run_rsa/f16/'\n",
    "\n",
    "- for each task epoch, e.g., responsePatterns.stim. It is a 1 x N structure, where N is the number of ROIs x number of sessions x number of participants. For example, if there are 7 ROIs, 1 participants and 2 sessions per participant, then we get a 1 x 14 structure. \n",
    "\n",
    "- Within each structure, there are several fields:\n",
    "    - 'name': ROI name | session | participant id\n",
    "    - 'data': a V x R x C matrix, where V is the number of voxels in that ROI, R is the number of runs (not trials, because we are using run-wise beta maps derived from first-level GLM), C is the number of conditions. For example, if there are 400 voxels in that ROI, 3 runs of beta maps per condition, and 8 conditions, then we get a 400 x 3 x 8 matrix.\n",
    "    \n",
    "You will get another .mat file under the '/mnt/Data1/linjdata1/vswmda/scan_data/rsa/beta_run_rsa/' folder:\n",
    "\n",
    "2) model.mat: Contains models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c506542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant imports\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import rsatoolbox\n",
    "import rsatoolbox.data as rsd # abbreviation to deal with dataset\n",
    "import rsatoolbox.rdm as rsr\n",
    "import os\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import math\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a923c93c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f87825b",
   "metadata": {},
   "source": [
    "# Define some functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa63633",
   "metadata": {},
   "source": [
    "# Get beta maps from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a0ea051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the working directory to be the timecourse data\n",
    "#os.chdir('/mnt/Data1/linjdata1/vswmda/scan_data/rsa/full_GLM_mgs_0.05_50/')\n",
    "#os.chdir('/gpfs/scratch/linjjiang/scan_data/rsa/full_GLM_mgs_0.05_50/')\n",
    "os.chdir('/gpfs/scratch/linjjiang/scan_data/rsa/full_GLM_atlas_roi/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5b6f588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/gpfs/scratch/linjjiang/scan_data/rsa/full_GLM_atlas_roi'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c3c3c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ROI name\n",
    "# # if full_GLM_atlas_roi\n",
    "order = ['area4-ju50',        \n",
    "          'v1-wang25','v2-wang25',\n",
    "          'ips0-wang15','ips1-wang15','ips2-wang15','ips3-wang15',\n",
    "          'ips4-wang15','ips5-wang15','spl1-wang15','ips-wang15',\n",
    "         'fef-wang25','spcs-md','ipcs-md','pmfg-md','amfg-md','ifg-md'\n",
    "          ] \n",
    "         #'area8-hcp','area9-hcp','area9|46-hcp','area44|45|47l-hcp','fef-hcp',\n",
    "\n",
    "\n",
    "# for atlas_roi:\n",
    "order_full = ['area4-ju50',        \n",
    "          'v1-wang25','v2-wang25',\n",
    "          'ips0-wang15','ips1-wang15','ips2-wang15','ips3-wang15',\n",
    "          'ips4-wang15','ips5-wang15','spl1-wang15','ips-wang15',\n",
    "         'fef-wang25','spcs-md','ipcs-md','pmfg-md','amfg-md','ifg-md',\n",
    "              'area8-hcp','area9-hcp','area9|46-hcp','area44|45|47l-hcp','fef-hcp'\n",
    "          ] \n",
    "         #'area8-hcp','area9-hcp','area9|46-hcp','area44|45|47l-hcp','fef-hcp',\n",
    "    \n",
    "    \n",
    "# if full_GLM_mgs_xxx\n",
    "# order = ['area4', 'v1', 'v2', 'ips', 'fef', 'sfg', 'mfg', 'ifg',\n",
    "#          'ips0', 'ips1', 'ips2', 'ips3', 'ips4', 'ips5', 'spl1']\n",
    "# for mgs roi:\n",
    "# order_full = order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5134167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['area4-ju50', 'v1-wang25', 'v2-wang25', 'ips0-wang15', 'ips1-wang15', 'ips2-wang15', 'ips3-wang15', 'ips4-wang15', 'ips5-wang15', 'spl1-wang15', 'ips-wang15', 'fef-wang25', 'spcs-md', 'ipcs-md', 'pmfg-md', 'amfg-md', 'ifg-md']\n"
     ]
    }
   ],
   "source": [
    "print(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a78c3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your specific order\n",
    "#order = ['area4', 'v1', 'v2', 'ips', 'fef', 'sfg', 'mfg', 'ifg']\n",
    "\n",
    "# Create a dictionary that maps each character to its position in the order\n",
    "\n",
    "order_dict = {char: index for index, char in enumerate(order_full)}\n",
    "\n",
    "# Group the strings into chunks\n",
    "def group_chunks(lst):\n",
    "    chunks = []\n",
    "    indices = []\n",
    "    current_chunk = []\n",
    "    current_indices = []\n",
    "    for i, elem in enumerate(lst):\n",
    "        if not current_chunk or elem == current_chunk[-1]:\n",
    "            current_chunk.append(elem)\n",
    "            current_indices.append(i)\n",
    "        else:\n",
    "            chunks.append(current_chunk)\n",
    "            indices.append(current_indices)\n",
    "            current_chunk = [elem]\n",
    "            current_indices = [i]\n",
    "    chunks.append(current_chunk)\n",
    "    indices.append(current_indices)\n",
    "    return chunks, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7993cb",
   "metadata": {},
   "source": [
    "### Load and save all the beta maps as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1d69834",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = ['f09','f10','f11','f12','f15','f16','f17','f18','f19']\n",
    "epochs = ['delay','response','stimulus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce783760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['R1', 'R1', 'R1']\n"
     ]
    }
   ],
   "source": [
    "run = 'R1'\n",
    "print([run]*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7d32a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(3):\n",
    "    for subject in subjects:\n",
    "        # load matlab data\n",
    "        measurements = io.matlab.loadmat(subject+'/responsePattern.mat')\n",
    "\n",
    "        # get data\n",
    "        responsePatterns = measurements['responsePatterns']\n",
    "\n",
    "        # get conditions\n",
    "        conditions = measurements['good_cond']\n",
    "        conditions = [\n",
    "            x\n",
    "            for xss in conditions\n",
    "            for xs in xss\n",
    "            for x in xs\n",
    "        ]\n",
    "\n",
    "        # get all data\n",
    "        data_all = responsePatterns[0][0][epoch][0] # all participants/sessions/rois for the delay period\n",
    "        #ndata = len(data_all['rawdata']) # how many datasets are there\n",
    "        \n",
    "        # all rois\n",
    "        name = data_all[:]['name']\n",
    "        roi_all = [name[0].split(sep=' | ')[0] for name in data_all[:]['name']]\n",
    "\n",
    "        # reorder the datasets by rois\n",
    "        chunks, indices = group_chunks(roi_all)\n",
    "\n",
    "        # Sort the chunks based on the custom order\n",
    "        sorted_chunk_indices = sorted(range(len(chunks)), key=lambda i: order_dict[chunks[i][0]])\n",
    "\n",
    "        # Generate the sorted list of indices\n",
    "        sorted_indices = [index for i in sorted_chunk_indices for index in indices[i]]\n",
    "\n",
    "        # now create a dataset object\n",
    "        dataset = []\n",
    "        for dd in sorted_indices:#range(0,ndata):\n",
    "            name = data_all[dd]['name']\n",
    "            name_spt = name[0].split(sep=' | ') \n",
    "            roi_label = name_spt[0] # roi label\n",
    "            subject = name_spt[1] # subject id\n",
    "            session = name_spt[2] # session id\n",
    "            run = name_spt[3] # run id\n",
    "            ecc_or_set = name_spt[4] # eccentricity or spatial set id (E1: 3dva; E2: 5.5dva; P1: spatial set1; P2: spatial set 2\n",
    "\n",
    "            if roi_label in ['area8-hcp','area9-hcp','area9|46-hcp','area44|45|47l-hcp','fef-hcp']:\n",
    "                continue\n",
    "                \n",
    "            data = data_all[dd]['rawdata'] # number of voxels by conditions\n",
    "            nVox = data.shape[0] # number of voxels\n",
    "            #nRun = data.shape[1] # number of trials\n",
    "            nCond = data.shape[1] # conditions\n",
    "            #data = data.reshape(nVox,nRun*nCond) # first condition, then trials\n",
    "            data = np.transpose(data) # number of conditions by voxels\n",
    "\n",
    "#             # which rows (conditions) do not have all nan values (valid conditions)\n",
    "#             cond_not_nan = ~np.isnan(data).all(axis=1)\n",
    "\n",
    "#             # then, remove columns (voxels) with any nan values during the valid conditions\n",
    "#             vox_not_nan = ~np.isnan(data[cond_not_nan,:]).any(axis=0) # voxels\n",
    "#             data = data[:,vox_not_nan]\n",
    "\n",
    "            # number of voxels included in the final analysis\n",
    "            nVox_not_nan = data.shape[1] # number of voxels   \n",
    "\n",
    "            obs_des = {'conds': conditions, \n",
    "                       'conds_index': np.arange(0,len(conditions)),\n",
    "                       'run_name': [run]*len(conditions)} # descriptors for observations\n",
    "            des = {'session': session, \n",
    "                   'subj': subject, \n",
    "                   'roi': roi_label, \n",
    "                   'run': run,\n",
    "                   'ecc_or_set': ecc_or_set,\n",
    "                   'name' : subject+' | '+session+' | '+roi_label} # descriptors #'run': run, 'name': name, 'ecc': ecc\n",
    "            chn_des = {'voxels': np.array(['voxel_' + str(x) for x in np.arange(nVox_not_nan)])} # descriptors for channels\n",
    "            dataset.append(rsd.Dataset(measurements=data,\n",
    "                               descriptors=des,\n",
    "                               obs_descriptors=obs_des,\n",
    "                               channel_descriptors=chn_des))\n",
    "        # save dataset\n",
    "        with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_run_ecc.pkg'),'wb') as f:\n",
    "            pickle.dump(dataset,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "887531f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5666bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12*len(order) # 12 runs by number of ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d4505c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rsatoolbox.data.Dataset\n",
      "measurements = \n",
      "[[ 1.13681543  1.38350403  3.93112111 ...  1.02807379  0.39928079\n",
      "   1.18702257]\n",
      " [-2.84869862 -4.16433764 -0.19244017 ...  1.57189214  0.55069697\n",
      "   0.33742237]\n",
      " [-3.5486486  -3.39419794  0.74993289 ...  2.44093609  1.17255092\n",
      "  -1.37427986]\n",
      " [-1.26268065 -6.55327177 -0.56980014 ... -0.48392674 -0.04410791\n",
      "  -1.77953053]\n",
      " [        nan         nan         nan ...         nan         nan\n",
      "          nan]]\n",
      "...\n",
      "\n",
      "descriptors: \n",
      "session = S5934\n",
      "subj = f19\n",
      "roi = area4-ju50\n",
      "run = R1\n",
      "ecc_or_set = P1\n",
      "name = f19 | S5934 | area4-ju50\n",
      "\n",
      "\n",
      "obs_descriptors: \n",
      "conds = ['CWL1', 'CTL1', 'CWR1', 'CTR1', 'CWL2', 'CTL2', 'CWR2', 'CTR2', 'NWL1', 'NTL1', 'NWR1', 'NTR1', 'NWL2', 'NTL2', 'NWR2', 'NTR2']\n",
      "conds_index = [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "run_name = ['R1', 'R1', 'R1', 'R1', 'R1', 'R1', 'R1', 'R1', 'R1', 'R1', 'R1', 'R1', 'R1', 'R1', 'R1', 'R1']\n",
      "\n",
      "\n",
      "channel_descriptors: \n",
      "voxels = ['voxel_0' 'voxel_1' 'voxel_2' ... 'voxel_1354' 'voxel_1355' 'voxel_1356']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf4611ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "[0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3]\n",
      "[0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "x = list(range(16))\n",
    "print(x)\n",
    "print(np.mod(x,4))\n",
    "print(np.mod(x,4).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ca64893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 0, 1, 2, 3, 4, 5, 6, 7, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "# Original list\n",
    "original_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "\n",
    "# Mapping list\n",
    "mapped_list = [0, 1, 2, 3, 0, 1, 2, 3, 4, 5, 6, 7, 4, 5, 6, 7]\n",
    "\n",
    "# Creating a dictionary for the mapping\n",
    "mapping_dict = dict(zip(original_list, mapped_list))\n",
    "\n",
    "# Apply the mapping\n",
    "result_list = [mapping_dict[item] for item in original_list]\n",
    "\n",
    "print(result_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01497543",
   "metadata": {},
   "source": [
    "### Merge datasets across runs for experiment 1 and 2\n",
    "#### this is for running the complex spatiotemporal model, as we combined two runs of different eccentricities, so each dataset\n",
    "#### will have all 8 spatiotemporal conditions\n",
    "SAME subject, session, ROI, pair of run would be the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fe18cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = ['f09','f10','f11','f12','f15','f16','f17','f18','f19']\n",
    "epochs = ['delay','response','stimulus']\n",
    "exp2_subjects = ['f17','f18','f19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c681840a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CWL1', 'CTL1', 'CWR1', 'CTR1', 'CWL2', 'CTL2', 'CWR2', 'CTR2', 'NWL1', 'NTL1', 'NWR1', 'NTR1', 'NWL2', 'NTL2', 'NWR2', 'NTR2']\n"
     ]
    }
   ],
   "source": [
    "# map the conditions\n",
    "\n",
    "# Original list\n",
    "original_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "\n",
    "# Mapping list\n",
    "mapped_list = [0, 1, 2, 3, 0, 1, 2, 3, 4, 5, 6, 7, 4, 5, 6, 7]\n",
    "\n",
    "# Creating a dictionary for the mapping\n",
    "mapping_dict = dict(zip(original_list, mapped_list))\n",
    "\n",
    "print(conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc0b6856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we only subset conditions, not voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38c95827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "for epoch in range(3):\n",
    "    for subject in subjects:\n",
    "        \n",
    "        # load dataset\n",
    "        with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_run_ecc.pkg'),'rb') as f:\n",
    "            dataset = pickle.load(f)\n",
    "        \n",
    "        # merge across runs\n",
    "        \n",
    "        # figure out which dataset belongs to the same session, subject, and roi\n",
    "        dicts = [dataset[i].descriptors['name'] for i in range(len(dataset))] # session, subject, roi for each dataset\n",
    "        \n",
    "        # Dictionary to store unique strings and their indices\n",
    "        unique_strings = {}\n",
    "\n",
    "        # Iterate through the list and populate the dictionary\n",
    "        for index, string in enumerate(dicts):\n",
    "            if string not in unique_strings:\n",
    "                unique_strings[string] = [index]\n",
    "            else:\n",
    "                unique_strings[string].append(index)\n",
    "                \n",
    "        dataset_merge = []        \n",
    "        for key in unique_strings: # for unique sessions and ROIs (subjects)\n",
    "            \n",
    "            # we want to further pair the runs across eccentricities\n",
    "            data_temp = copy.deepcopy([dataset[k] for k in unique_strings[key]])\n",
    "            if subject in exp2_subjects:\n",
    "                dataset1 = [d for d in data_temp if (d.descriptors['ecc_or_set'] == 'P1')] # ecc 1\n",
    "                dataset2 = [d for d in data_temp if (d.descriptors['ecc_or_set'] == 'P2')] # ecc 2\n",
    "            else:\n",
    "                dataset1 = [d for d in data_temp if (d.descriptors['ecc_or_set'] == 'E1')] # ecc 1\n",
    "                dataset2 = [d for d in data_temp if (d.descriptors['ecc_or_set'] == 'E2')] # ecc 2\n",
    "            \n",
    "            ndata = min(len(dataset1),len(dataset2))\n",
    "            \n",
    "            # merge those datasets\n",
    "            for r in range(ndata):\n",
    "                run_val1 = dataset1[r].descriptors.pop('run', None)\n",
    "                run_val2 = dataset2[r].descriptors.pop('run', None) #ndata-r-1\n",
    "                ecc_val1 = dataset1[r].descriptors.pop('ecc_or_set', None)\n",
    "                ecc_val2 = dataset2[r].descriptors.pop('ecc_or_set', None) #ndata-r-1\n",
    "                \n",
    "                dataset_test = rsatoolbox.data.dataset.merge_subsets([dataset1[r],dataset2[r]]) #ndata-r-1\n",
    "\n",
    "                dataset_test.descriptors['run'] = run_val1+run_val2\n",
    "                dataset_test.descriptors['ecc_or_set'] = ecc_val1+ecc_val2\n",
    "\n",
    "                # remove nans\n",
    "                measures = dataset_test.get_measurements()\n",
    "\n",
    "                # which rows (conditions) do not have all nan values (valid conditions)\n",
    "                cond_not_nan = ~np.isnan(measures).all(axis=1)\n",
    "\n",
    "                # use it to subset data (measurements)\n",
    "                measures = measures[cond_not_nan,:]\n",
    "\n",
    "                # subset conditions\n",
    "                conds = dataset_test.obs_descriptors['conds']\n",
    "                conds_index = dataset_test.obs_descriptors['conds_index']\n",
    "                run_name = dataset_test.obs_descriptors['run_name']\n",
    "                \n",
    "                subset_conds = [cond for cond, flag in zip(conds, cond_not_nan) if flag]\n",
    "                subset_conds_index = [cond for cond, flag in zip(conds_index, cond_not_nan) if flag]\n",
    "                subset_run_name = [cond for cond, flag in zip(run_name, cond_not_nan) if flag]\n",
    "                \n",
    "                # if it is experiment 2, we also want to simplify the conditions\n",
    "                if subject in ['f17','f18','f19']:\n",
    "                    subset_conds = [c[:3] for c in subset_conds]\n",
    "                    #subset_conds_index = np.mod(subset_conds_index,8).tolist()\n",
    "                    # Apply the mapping\n",
    "                    subset_conds_index = [mapping_dict[item] for item in subset_conds_index]\n",
    "                    \n",
    "                # assign the subset conditions and measurements to the merged dataset\n",
    "                dataset_test.obs_descriptors['conds'] = subset_conds\n",
    "                dataset_test.obs_descriptors['conds_index'] = subset_conds_index\n",
    "                dataset_test.obs_descriptors['run_name'] = subset_run_name\n",
    "                dataset_test.measurements = measures\n",
    "\n",
    "                # append the merged dataset\n",
    "                dataset_merge.append(dataset_test)\n",
    " \n",
    "         # save dataset\n",
    "        with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_merge_across_run.pkg'),'wb') as f:\n",
    "            pickle.dump(dataset_merge,f)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14a13cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_merge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a47067bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6*len(order) # we combined 2 runs together, so the number of dataset is divided by 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24101da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rsatoolbox.data.Dataset\n",
      "measurements = \n",
      "[[ 1.13681543  1.38350403  3.93112111 ...  1.02807379  0.39928079\n",
      "   1.18702257]\n",
      " [-2.84869862 -4.16433764 -0.19244017 ...  1.57189214  0.55069697\n",
      "   0.33742237]\n",
      " [-3.5486486  -3.39419794  0.74993289 ...  2.44093609  1.17255092\n",
      "  -1.37427986]\n",
      " [-1.26268065 -6.55327177 -0.56980014 ... -0.48392674 -0.04410791\n",
      "  -1.77953053]\n",
      " [-1.64067352 -2.59643316 -1.58815801 ...  2.16497278  1.5415889\n",
      "  -1.1298852 ]]\n",
      "...\n",
      "\n",
      "descriptors: \n",
      "session = S5934\n",
      "roi = area4-ju50\n",
      "subj = f19\n",
      "name = f19 | S5934 | area4-ju50\n",
      "run = R1R2\n",
      "ecc_or_set = P1P2\n",
      "\n",
      "\n",
      "obs_descriptors: \n",
      "conds_index = [0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7]\n",
      "run_name = ['R1', 'R1', 'R1', 'R1', 'R1', 'R1', 'R1', 'R1', 'R2', 'R2', 'R2', 'R2', 'R2', 'R2', 'R2', 'R2']\n",
      "conds = ['CWL', 'CTL', 'CWR', 'CTR', 'NWL', 'NTL', 'NWR', 'NTR', 'CWL', 'CTL', 'CWR', 'CTR', 'NWL', 'NTL', 'NWR', 'NTR']\n",
      "\n",
      "\n",
      "channel_descriptors: \n",
      "voxels = ['voxel_0' 'voxel_1' 'voxel_2' ... 'voxel_1354' 'voxel_1355' 'voxel_1356']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset_merge[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2477acb",
   "metadata": {},
   "source": [
    "### get rid of spatial set condition for experiment 2\n",
    "#### this is for running the complex spatiotemporal model, as we get rid of spatial sets from the current conditions\n",
    "SAME subject, session, ROI, pair of run, would be the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5a94ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2_subjects = ['f17','f18','f19']\n",
    "epochs = ['delay','response','stimulus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f01ffe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(3):\n",
    "    for subject in exp2_subjects:\n",
    "        \n",
    "        # load dataset\n",
    "        with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_run_ecc.pkg'),'rb') as f:\n",
    "            dataset = pickle.load(f)\n",
    "            \n",
    "        dataset_simple_cond = []\n",
    "        for data in dataset:\n",
    "            dataset1 = copy.deepcopy(data) \n",
    "\n",
    "            # remove nans\n",
    "            measures = dataset1.get_measurements()\n",
    "\n",
    "            # which rows (conditions) do not have all nan values (valid conditions)\n",
    "            cond_not_nan = ~np.isnan(measures).all(axis=1)\n",
    "\n",
    "            #print(cond_not_nan)\n",
    "\n",
    "            # use it to subset data (measurements)\n",
    "            measures = measures[cond_not_nan,:]\n",
    "\n",
    "            # subset conditions\n",
    "            conds = dataset1.obs_descriptors['conds']\n",
    "            conds_index = dataset1.obs_descriptors['conds_index']\n",
    "            subset_conds = [cond[:3] for cond, flag in zip(conds, cond_not_nan) if flag]\n",
    "            subset_conds_index = [c for c in range(len(subset_conds))] #[cond for cond, flag in zip(conds_index, cond_not_nan) if flag]\n",
    "\n",
    "            # assign the subset conditions and measurements to the merged dataset\n",
    "            dataset1.obs_descriptors['conds'] = subset_conds\n",
    "            dataset1.obs_descriptors['conds_index'] = subset_conds_index\n",
    "            dataset1.measurements = measures\n",
    "\n",
    "            # append the dataset\n",
    "            dataset_simple_cond.append(dataset1)\n",
    " \n",
    "         # save dataset\n",
    "        with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_simple_cond_exp2.pkg'),'wb') as f:\n",
    "            pickle.dump(dataset_simple_cond,f)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5adb6cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_simple_cond))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "985af155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rsatoolbox.data.Dataset\n",
      "measurements = \n",
      "[[-4.32295465 -6.16556215  0.7349382  ... -1.17331648  0.45550582\n",
      "  -2.78499293]\n",
      " [-1.72085452 -3.63415432 -1.65540874 ...  0.33521384 -0.54659218\n",
      "  -0.26850107]\n",
      " [-3.02165127 -6.29844046 -1.63729489 ...  0.08400786  1.30360365\n",
      "  -1.44681382]\n",
      " [-3.38832998 -6.20992613  0.13150956 ...  0.90429479 -2.20506287\n",
      "  -4.67829561]\n",
      " [-3.95717478 -6.65573072 -0.20510076 ... -3.87506557 -1.18707049\n",
      "  -2.55028176]]\n",
      "...\n",
      "\n",
      "descriptors: \n",
      "session = S5934\n",
      "subj = f19\n",
      "roi = area4-ju50\n",
      "run = R2\n",
      "ecc_or_set = P2\n",
      "name = f19 | S5934 | area4-ju50\n",
      "\n",
      "\n",
      "obs_descriptors: \n",
      "conds = ['CWL', 'CTL', 'CWR', 'CTR', 'NWL', 'NTL', 'NWR', 'NTR']\n",
      "conds_index = [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "run_name = ['R2', 'R2', 'R2', 'R2', 'R2', 'R2', 'R2', 'R2', 'R2', 'R2', 'R2', 'R2', 'R2', 'R2', 'R2', 'R2']\n",
      "\n",
      "\n",
      "channel_descriptors: \n",
      "voxels = ['voxel_0' 'voxel_1' 'voxel_2' ... 'voxel_1354' 'voxel_1355' 'voxel_1356']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset_simple_cond[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c62941e",
   "metadata": {},
   "source": [
    "## Merge dataset for complex model (e.g., 8 spatiotemporal category)\n",
    "Here we need to subset voxels. \n",
    "It's time to load residual and subset voxels for residual too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6340a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to further merge the dataset, so that we have different sessions and runs in one matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df22c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = ['f09','f10','f11','f12','f15','f16','f17','f18','f19']\n",
    "epochs = ['delay','response','stimulus']\n",
    "exp2_subjects = ['f17','f18','f19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "394d769e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2490, 1339) 1339\n",
      "(2490, 1743) 1743\n",
      "(2490, 1154) 1154\n",
      "(2490, 1597) 1597\n",
      "(2490, 992) 992\n",
      "(2490, 758) 758\n",
      "(2490, 491) 491\n",
      "(2490, 439) 439\n",
      "(2490, 404) 404\n",
      "(2490, 579) 579\n",
      "(2490, 4119) 4119\n",
      "(2490, 1033) 1033\n",
      "(2490, 1939) 1939\n",
      "(2490, 1891) 1891\n",
      "(2490, 1352) 1352\n",
      "(2490, 1240) 1240\n",
      "(2490, 792) 792\n",
      "(2490, 1248) 1248\n",
      "(2075, 1248) 1248\n",
      "(2490, 1728) 1728\n",
      "(2075, 1728) 1728\n",
      "(2490, 1417) 1417\n",
      "(2075, 1417) 1417\n",
      "(2490, 1630) 1630\n",
      "(2075, 1630) 1630\n",
      "(2490, 994) 994\n",
      "(2075, 994) 994\n",
      "(2490, 764) 764\n",
      "(2075, 764) 764\n",
      "(2490, 489) 489\n",
      "(2075, 489) 489\n",
      "(2490, 439) 439\n",
      "(2075, 439) 439\n",
      "(2490, 404) 404\n",
      "(2075, 404) 404\n",
      "(2490, 585) 585\n",
      "(2075, 585) 585\n",
      "(2490, 4165) 4165\n",
      "(2075, 4165) 4165\n",
      "(2490, 840) 840\n",
      "(2075, 840) 840\n",
      "(2490, 1669) 1669\n",
      "(2075, 1669) 1669\n",
      "(2490, 1637) 1637\n",
      "(2075, 1637) 1637\n",
      "(2490, 1095) 1095\n",
      "(2075, 1095) 1095\n",
      "(2490, 1147) 1147\n",
      "(2075, 1147) 1147\n",
      "(2490, 356) 356\n",
      "(2075, 356) 356\n",
      "(2075, 1351) 1351\n",
      "(2905, 1351) 1351\n",
      "(2075, 1766) 1766\n",
      "(2905, 1766) 1766\n",
      "(2075, 1565) 1565\n",
      "(2905, 1565) 1565\n",
      "(2075, 1636) 1636\n",
      "(2905, 1636) 1636\n",
      "(2075, 992) 992\n",
      "(2905, 992) 992\n",
      "(2075, 764) 764\n",
      "(2905, 764) 764\n",
      "(2075, 491) 491\n",
      "(2905, 491) 491\n",
      "(2075, 439) 439\n",
      "(2905, 439) 439\n",
      "(2075, 404) 404\n",
      "(2905, 404) 404\n",
      "(2075, 577) 577\n",
      "(2905, 577) 577\n",
      "(2075, 4165) 4165\n",
      "(2905, 4165) 4165\n",
      "(2075, 1071) 1071\n",
      "(2905, 1071) 1071\n",
      "(2075, 2164) 2164\n",
      "(2905, 2164) 2164\n",
      "(2075, 2213) 2213\n",
      "(2905, 2213) 2213\n",
      "(2075, 420) 420\n",
      "(2905, 420) 420\n",
      "(2075, 0) 0\n",
      "(2905, 0) 0\n",
      "(2075, 146) 146\n",
      "(2905, 146) 146\n",
      "(2490, 1357) 1357\n",
      "(2490, 1357) 1357\n",
      "(2490, 1891) 1891\n",
      "(2490, 1891) 1891\n",
      "(2490, 1463) 1463\n",
      "(2490, 1463) 1463\n",
      "(2490, 1638) 1638\n",
      "(2490, 1638) 1638\n",
      "(2490, 994) 994\n",
      "(2490, 994) 994\n",
      "(2490, 766) 766\n",
      "(2490, 766) 766\n",
      "(2490, 487) 487\n",
      "(2490, 487) 487\n",
      "(2490, 439) 439\n",
      "(2490, 439) 439\n",
      "(2490, 404) 404\n",
      "(2490, 404) 404\n",
      "(2490, 527) 527\n",
      "(2490, 527) 527\n",
      "(2490, 4113) 4113\n",
      "(2490, 4113) 4113\n",
      "(2490, 892) 892\n",
      "(2490, 892) 892\n",
      "(2490, 1759) 1759\n",
      "(2490, 1759) 1759\n",
      "(2490, 1742) 1742\n",
      "(2490, 1742) 1742\n",
      "(2490, 1339) 1339\n",
      "(2490, 1339) 1339\n",
      "(2490, 1242) 1242\n",
      "(2490, 1242) 1242\n",
      "(2490, 589) 589\n",
      "(2490, 589) 589\n",
      "(2490, 1345) 1345\n",
      "(2490, 1345) 1345\n",
      "(2490, 1724) 1724\n",
      "(2490, 1724) 1724\n",
      "(2490, 1538) 1538\n",
      "(2490, 1538) 1538\n",
      "(2490, 1650) 1650\n",
      "(2490, 1650) 1650\n",
      "(2490, 992) 992\n",
      "(2490, 992) 992\n",
      "(2490, 756) 756\n",
      "(2490, 756) 756\n",
      "(2490, 491) 491\n",
      "(2490, 491) 491\n",
      "(2490, 439) 439\n",
      "(2490, 439) 439\n",
      "(2490, 404) 404\n",
      "(2490, 404) 404\n",
      "(2490, 555) 555\n",
      "(2490, 555) 555\n",
      "(2490, 4145) 4145\n",
      "(2490, 4145) 4145\n",
      "(2490, 1089) 1089\n",
      "(2490, 1089) 1089\n",
      "(2490, 2226) 2226\n",
      "(2490, 2226) 2226\n",
      "(2490, 2226) 2226\n",
      "(2490, 2226) 2226\n",
      "(2490, 1410) 1410\n",
      "(2490, 1410) 1410\n",
      "(2490, 1242) 1242\n",
      "(2490, 1242) 1242\n",
      "(2490, 881) 881\n",
      "(2490, 881) 881\n",
      "(2490, 1316) 1316\n",
      "(2490, 1316) 1316\n",
      "(2490, 1250) 1250\n",
      "(2490, 1250) 1250\n",
      "(2490, 1017) 1017\n",
      "(2490, 1017) 1017\n",
      "(2490, 1650) 1650\n",
      "(2490, 1650) 1650\n",
      "(2490, 994) 994\n",
      "(2490, 994) 994\n",
      "(2490, 766) 766\n",
      "(2490, 766) 766\n",
      "(2490, 491) 491\n",
      "(2490, 491) 491\n",
      "(2490, 439) 439\n",
      "(2490, 439) 439\n",
      "(2490, 404) 404\n",
      "(2490, 404) 404\n",
      "(2490, 557) 557\n",
      "(2490, 557) 557\n",
      "(2490, 4161) 4161\n",
      "(2490, 4161) 4161\n",
      "(2490, 1025) 1025\n",
      "(2490, 1025) 1025\n",
      "(2490, 1907) 1907\n",
      "(2490, 1907) 1907\n",
      "(2490, 1733) 1733\n",
      "(2490, 1733) 1733\n",
      "(2490, 1424) 1424\n",
      "(2490, 1424) 1424\n",
      "(2490, 1242) 1242\n",
      "(2490, 1242) 1242\n",
      "(2490, 1324) 1324\n",
      "(2490, 1324) 1324\n",
      "(2490, 1286) 1286\n",
      "(2490, 1286) 1286\n",
      "(2490, 1299) 1299\n",
      "(2490, 1299) 1299\n",
      "(2490, 1131) 1131\n",
      "(2490, 1131) 1131\n",
      "(2490, 1650) 1650\n",
      "(2490, 1650) 1650\n",
      "(2490, 992) 992\n",
      "(2490, 992) 992\n",
      "(2490, 764) 764\n",
      "(2490, 764) 764\n",
      "(2490, 491) 491\n",
      "(2490, 491) 491\n",
      "(2490, 439) 439\n",
      "(2490, 439) 439\n",
      "(2490, 404) 404\n",
      "(2490, 404) 404\n",
      "(2490, 585) 585\n",
      "(2490, 585) 585\n",
      "(2490, 4185) 4185\n",
      "(2490, 4185) 4185\n",
      "(2490, 798) 798\n",
      "(2490, 798) 798\n",
      "(2490, 1478) 1478\n",
      "(2490, 1478) 1478\n",
      "(2490, 1693) 1693\n",
      "(2490, 1693) 1693\n",
      "(2490, 902) 902\n",
      "(2490, 902) 902\n",
      "(2490, 853) 853\n",
      "(2490, 853) 853\n",
      "(2490, 443) 443\n",
      "(2490, 443) 443\n",
      "(2490, 1293) 1293\n",
      "(2490, 1293) 1293\n",
      "(2490, 1190) 1190\n",
      "(2490, 1190) 1190\n",
      "(2490, 844) 844\n",
      "(2490, 844) 844\n",
      "(2490, 1400) 1400\n",
      "(2490, 1400) 1400\n",
      "(2490, 917) 917\n",
      "(2490, 917) 917\n",
      "(2490, 742) 742\n",
      "(2490, 742) 742\n",
      "(2490, 487) 487\n",
      "(2490, 487) 487\n",
      "(2490, 439) 439\n",
      "(2490, 439) 439\n",
      "(2490, 404) 404\n",
      "(2490, 404) 404\n",
      "(2490, 573) 573\n",
      "(2490, 573) 573\n",
      "(2490, 3859) 3859\n",
      "(2490, 3859) 3859\n",
      "(2490, 923) 923\n",
      "(2490, 923) 923\n",
      "(2490, 1718) 1718\n",
      "(2490, 1718) 1718\n",
      "(2490, 1797) 1797\n",
      "(2490, 1797) 1797\n",
      "(2490, 1149) 1149\n",
      "(2490, 1149) 1149\n",
      "(2490, 1209) 1209\n",
      "(2490, 1209) 1209\n",
      "(2490, 685) 685\n",
      "(2490, 685) 685\n",
      "(2490, 1357) 1357\n",
      "(2490, 1357) 1357\n",
      "(2490, 1425) 1425\n",
      "(2490, 1425) 1425\n",
      "(2490, 1142) 1142\n",
      "(2490, 1142) 1142\n",
      "(2490, 1632) 1632\n",
      "(2490, 1632) 1632\n",
      "(2490, 994) 994\n",
      "(2490, 994) 994\n",
      "(2490, 764) 764\n",
      "(2490, 764) 764\n",
      "(2490, 491) 491\n",
      "(2490, 491) 491\n",
      "(2490, 439) 439\n",
      "(2490, 439) 439\n",
      "(2490, 404) 404\n",
      "(2490, 404) 404\n",
      "(2490, 575) 575\n",
      "(2490, 575) 575\n",
      "(2490, 4159) 4159\n",
      "(2490, 4159) 4159\n",
      "(2490, 1091) 1091\n",
      "(2490, 1091) 1091\n",
      "(2490, 2264) 2264\n",
      "(2490, 2264) 2264\n",
      "(2490, 2092) 2092\n",
      "(2490, 2092) 2092\n",
      "(2490, 1288) 1288\n",
      "(2490, 1288) 1288\n",
      "(2490, 1240) 1240\n",
      "(2490, 1240) 1240\n",
      "(2490, 743) 743\n",
      "(2490, 743) 743\n",
      "(2490, 1339) 1339\n",
      "(2490, 1743) 1743\n",
      "(2490, 1154) 1154\n",
      "(2490, 1597) 1597\n",
      "(2490, 992) 992\n",
      "(2490, 758) 758\n",
      "(2490, 491) 491\n",
      "(2490, 439) 439\n",
      "(2490, 404) 404\n",
      "(2490, 579) 579\n",
      "(2490, 4119) 4119\n",
      "(2490, 1033) 1033\n",
      "(2490, 1939) 1939\n",
      "(2490, 1891) 1891\n",
      "(2490, 1352) 1352\n",
      "(2490, 1240) 1240\n",
      "(2490, 792) 792\n",
      "(2490, 1248) 1248\n",
      "(2075, 1248) 1248\n",
      "(2490, 1728) 1728\n",
      "(2075, 1728) 1728\n",
      "(2490, 1417) 1417\n",
      "(2075, 1417) 1417\n",
      "(2490, 1630) 1630\n",
      "(2075, 1630) 1630\n",
      "(2490, 994) 994\n",
      "(2075, 994) 994\n",
      "(2490, 764) 764\n",
      "(2075, 764) 764\n",
      "(2490, 489) 489\n",
      "(2075, 489) 489\n",
      "(2490, 439) 439\n",
      "(2075, 439) 439\n",
      "(2490, 404) 404\n",
      "(2075, 404) 404\n",
      "(2490, 585) 585\n",
      "(2075, 585) 585\n",
      "(2490, 4165) 4165\n",
      "(2075, 4165) 4165\n",
      "(2490, 840) 840\n",
      "(2075, 840) 840\n",
      "(2490, 1669) 1669\n",
      "(2075, 1669) 1669\n",
      "(2490, 1637) 1637\n",
      "(2075, 1637) 1637\n",
      "(2490, 1095) 1095\n",
      "(2075, 1095) 1095\n",
      "(2490, 1147) 1147\n",
      "(2075, 1147) 1147\n",
      "(2490, 356) 356\n",
      "(2075, 356) 356\n",
      "(2075, 1351) 1351\n",
      "(2905, 1351) 1351\n",
      "(2075, 1766) 1766\n",
      "(2905, 1766) 1766\n",
      "(2075, 1565) 1565\n",
      "(2905, 1565) 1565\n",
      "(2075, 1636) 1636\n",
      "(2905, 1636) 1636\n",
      "(2075, 992) 992\n",
      "(2905, 992) 992\n",
      "(2075, 764) 764\n",
      "(2905, 764) 764\n",
      "(2075, 491) 491\n",
      "(2905, 491) 491\n",
      "(2075, 439) 439\n",
      "(2905, 439) 439\n",
      "(2075, 404) 404\n",
      "(2905, 404) 404\n",
      "(2075, 577) 577\n",
      "(2905, 577) 577\n",
      "(2075, 4165) 4165\n",
      "(2905, 4165) 4165\n",
      "(2075, 1071) 1071\n",
      "(2905, 1071) 1071\n",
      "(2075, 2164) 2164\n",
      "(2905, 2164) 2164\n",
      "(2075, 2213) 2213\n",
      "(2905, 2213) 2213\n",
      "(2075, 420) 420\n",
      "(2905, 420) 420\n",
      "(2075, 0) 0\n",
      "(2905, 0) 0\n",
      "(2075, 146) 146\n",
      "(2905, 146) 146\n",
      "(2490, 1357) 1357\n",
      "(2490, 1357) 1357\n",
      "(2490, 1891) 1891\n",
      "(2490, 1891) 1891\n",
      "(2490, 1463) 1463\n",
      "(2490, 1463) 1463\n",
      "(2490, 1638) 1638\n",
      "(2490, 1638) 1638\n",
      "(2490, 994) 994\n",
      "(2490, 994) 994\n",
      "(2490, 766) 766\n",
      "(2490, 766) 766\n",
      "(2490, 487) 487\n",
      "(2490, 487) 487\n",
      "(2490, 439) 439\n",
      "(2490, 439) 439\n",
      "(2490, 404) 404\n",
      "(2490, 404) 404\n",
      "(2490, 527) 527\n",
      "(2490, 527) 527\n",
      "(2490, 4113) 4113\n",
      "(2490, 4113) 4113\n",
      "(2490, 892) 892\n",
      "(2490, 892) 892\n",
      "(2490, 1759) 1759\n",
      "(2490, 1759) 1759\n",
      "(2490, 1742) 1742\n",
      "(2490, 1742) 1742\n",
      "(2490, 1339) 1339\n",
      "(2490, 1339) 1339\n",
      "(2490, 1242) 1242\n",
      "(2490, 1242) 1242\n",
      "(2490, 589) 589\n",
      "(2490, 589) 589\n",
      "(2490, 1345) 1345\n",
      "(2490, 1345) 1345\n",
      "(2490, 1724) 1724\n",
      "(2490, 1724) 1724\n",
      "(2490, 1538) 1538\n",
      "(2490, 1538) 1538\n",
      "(2490, 1650) 1650\n",
      "(2490, 1650) 1650\n",
      "(2490, 992) 992\n",
      "(2490, 992) 992\n",
      "(2490, 756) 756\n",
      "(2490, 756) 756\n",
      "(2490, 491) 491\n",
      "(2490, 491) 491\n",
      "(2490, 439) 439\n",
      "(2490, 439) 439\n",
      "(2490, 404) 404\n",
      "(2490, 404) 404\n",
      "(2490, 555) 555\n",
      "(2490, 555) 555\n",
      "(2490, 4145) 4145\n",
      "(2490, 4145) 4145\n",
      "(2490, 1089) 1089\n",
      "(2490, 1089) 1089\n",
      "(2490, 2226) 2226\n",
      "(2490, 2226) 2226\n",
      "(2490, 2226) 2226\n",
      "(2490, 2226) 2226\n",
      "(2490, 1410) 1410\n",
      "(2490, 1410) 1410\n",
      "(2490, 1242) 1242\n",
      "(2490, 1242) 1242\n",
      "(2490, 881) 881\n",
      "(2490, 881) 881\n",
      "(2490, 1316) 1316\n",
      "(2490, 1316) 1316\n",
      "(2490, 1250) 1250\n",
      "(2490, 1250) 1250\n",
      "(2490, 1017) 1017\n",
      "(2490, 1017) 1017\n",
      "(2490, 1650) 1650\n",
      "(2490, 1650) 1650\n",
      "(2490, 994) 994\n",
      "(2490, 994) 994\n",
      "(2490, 766) 766\n",
      "(2490, 766) 766\n",
      "(2490, 491) 491\n",
      "(2490, 491) 491\n",
      "(2490, 439) 439\n",
      "(2490, 439) 439\n",
      "(2490, 404) 404\n",
      "(2490, 404) 404\n",
      "(2490, 557) 557\n",
      "(2490, 557) 557\n",
      "(2490, 4161) 4161\n",
      "(2490, 4161) 4161\n",
      "(2490, 1025) 1025\n",
      "(2490, 1025) 1025\n",
      "(2490, 1907) 1907\n",
      "(2490, 1907) 1907\n",
      "(2490, 1733) 1733\n",
      "(2490, 1733) 1733\n",
      "(2490, 1424) 1424\n",
      "(2490, 1424) 1424\n",
      "(2490, 1242) 1242\n",
      "(2490, 1242) 1242\n",
      "(2490, 1324) 1324\n",
      "(2490, 1324) 1324\n",
      "(2490, 1286) 1286\n",
      "(2490, 1286) 1286\n",
      "(2490, 1299) 1299\n",
      "(2490, 1299) 1299\n",
      "(2490, 1131) 1131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2490, 1131) 1131\n",
      "(2490, 1650) 1650\n",
      "(2490, 1650) 1650\n",
      "(2490, 992) 992\n",
      "(2490, 992) 992\n",
      "(2490, 764) 764\n",
      "(2490, 764) 764\n",
      "(2490, 491) 491\n",
      "(2490, 491) 491\n",
      "(2490, 439) 439\n",
      "(2490, 439) 439\n",
      "(2490, 404) 404\n",
      "(2490, 404) 404\n",
      "(2490, 585) 585\n",
      "(2490, 585) 585\n",
      "(2490, 4185) 4185\n",
      "(2490, 4185) 4185\n",
      "(2490, 798) 798\n",
      "(2490, 798) 798\n",
      "(2490, 1478) 1478\n",
      "(2490, 1478) 1478\n",
      "(2490, 1693) 1693\n",
      "(2490, 1693) 1693\n",
      "(2490, 902) 902\n",
      "(2490, 902) 902\n",
      "(2490, 853) 853\n",
      "(2490, 853) 853\n",
      "(2490, 443) 443\n",
      "(2490, 443) 443\n",
      "(2490, 1293) 1293\n",
      "(2490, 1293) 1293\n",
      "(2490, 1190) 1190\n",
      "(2490, 1190) 1190\n",
      "(2490, 844) 844\n",
      "(2490, 844) 844\n",
      "(2490, 1400) 1400\n",
      "(2490, 1400) 1400\n",
      "(2490, 917) 917\n",
      "(2490, 917) 917\n",
      "(2490, 742) 742\n",
      "(2490, 742) 742\n",
      "(2490, 487) 487\n",
      "(2490, 487) 487\n",
      "(2490, 439) 439\n",
      "(2490, 439) 439\n",
      "(2490, 404) 404\n",
      "(2490, 404) 404\n",
      "(2490, 573) 573\n",
      "(2490, 573) 573\n",
      "(2490, 3859) 3859\n",
      "(2490, 3859) 3859\n",
      "(2490, 923) 923\n",
      "(2490, 923) 923\n",
      "(2490, 1718) 1718\n",
      "(2490, 1718) 1718\n",
      "(2490, 1797) 1797\n",
      "(2490, 1797) 1797\n",
      "(2490, 1149) 1149\n",
      "(2490, 1149) 1149\n",
      "(2490, 1209) 1209\n",
      "(2490, 1209) 1209\n",
      "(2490, 685) 685\n",
      "(2490, 685) 685\n",
      "(2490, 1357) 1357\n",
      "(2490, 1357) 1357\n",
      "(2490, 1425) 1425\n",
      "(2490, 1425) 1425\n",
      "(2490, 1142) 1142\n",
      "(2490, 1142) 1142\n",
      "(2490, 1632) 1632\n",
      "(2490, 1632) 1632\n",
      "(2490, 994) 994\n",
      "(2490, 994) 994\n",
      "(2490, 764) 764\n",
      "(2490, 764) 764\n",
      "(2490, 491) 491\n",
      "(2490, 491) 491\n",
      "(2490, 439) 439\n",
      "(2490, 439) 439\n",
      "(2490, 404) 404\n",
      "(2490, 404) 404\n",
      "(2490, 575) 575\n",
      "(2490, 575) 575\n",
      "(2490, 4159) 4159\n",
      "(2490, 4159) 4159\n",
      "(2490, 1091) 1091\n",
      "(2490, 1091) 1091\n",
      "(2490, 2264) 2264\n",
      "(2490, 2264) 2264\n",
      "(2490, 2092) 2092\n",
      "(2490, 2092) 2092\n",
      "(2490, 1288) 1288\n",
      "(2490, 1288) 1288\n",
      "(2490, 1240) 1240\n",
      "(2490, 1240) 1240\n",
      "(2490, 743) 743\n",
      "(2490, 743) 743\n",
      "(2490, 1339) 1339\n",
      "(2490, 1743) 1743\n",
      "(2490, 1154) 1154\n",
      "(2490, 1597) 1597\n",
      "(2490, 992) 992\n",
      "(2490, 758) 758\n",
      "(2490, 491) 491\n",
      "(2490, 439) 439\n",
      "(2490, 404) 404\n",
      "(2490, 579) 579\n",
      "(2490, 4119) 4119\n",
      "(2490, 1033) 1033\n",
      "(2490, 1939) 1939\n",
      "(2490, 1891) 1891\n",
      "(2490, 1352) 1352\n",
      "(2490, 1240) 1240\n",
      "(2490, 792) 792\n",
      "(2490, 1248) 1248\n",
      "(2075, 1248) 1248\n",
      "(2490, 1728) 1728\n",
      "(2075, 1728) 1728\n",
      "(2490, 1417) 1417\n",
      "(2075, 1417) 1417\n",
      "(2490, 1630) 1630\n",
      "(2075, 1630) 1630\n",
      "(2490, 994) 994\n",
      "(2075, 994) 994\n",
      "(2490, 764) 764\n",
      "(2075, 764) 764\n",
      "(2490, 489) 489\n",
      "(2075, 489) 489\n",
      "(2490, 439) 439\n",
      "(2075, 439) 439\n",
      "(2490, 404) 404\n",
      "(2075, 404) 404\n",
      "(2490, 585) 585\n",
      "(2075, 585) 585\n",
      "(2490, 4165) 4165\n",
      "(2075, 4165) 4165\n",
      "(2490, 840) 840\n",
      "(2075, 840) 840\n",
      "(2490, 1669) 1669\n",
      "(2075, 1669) 1669\n",
      "(2490, 1637) 1637\n",
      "(2075, 1637) 1637\n",
      "(2490, 1095) 1095\n",
      "(2075, 1095) 1095\n",
      "(2490, 1147) 1147\n",
      "(2075, 1147) 1147\n",
      "(2490, 356) 356\n",
      "(2075, 356) 356\n",
      "(2075, 1351) 1351\n",
      "(2905, 1351) 1351\n",
      "(2075, 1766) 1766\n",
      "(2905, 1766) 1766\n",
      "(2075, 1565) 1565\n",
      "(2905, 1565) 1565\n",
      "(2075, 1636) 1636\n",
      "(2905, 1636) 1636\n",
      "(2075, 992) 992\n",
      "(2905, 992) 992\n",
      "(2075, 764) 764\n",
      "(2905, 764) 764\n",
      "(2075, 491) 491\n",
      "(2905, 491) 491\n",
      "(2075, 439) 439\n",
      "(2905, 439) 439\n",
      "(2075, 404) 404\n",
      "(2905, 404) 404\n",
      "(2075, 577) 577\n",
      "(2905, 577) 577\n",
      "(2075, 4165) 4165\n",
      "(2905, 4165) 4165\n",
      "(2075, 1071) 1071\n",
      "(2905, 1071) 1071\n",
      "(2075, 2164) 2164\n",
      "(2905, 2164) 2164\n",
      "(2075, 2213) 2213\n",
      "(2905, 2213) 2213\n",
      "(2075, 420) 420\n",
      "(2905, 420) 420\n",
      "(2075, 0) 0\n",
      "(2905, 0) 0\n",
      "(2075, 146) 146\n",
      "(2905, 146) 146\n",
      "(2490, 1357) 1357\n",
      "(2490, 1357) 1357\n",
      "(2490, 1891) 1891\n",
      "(2490, 1891) 1891\n",
      "(2490, 1463) 1463\n",
      "(2490, 1463) 1463\n",
      "(2490, 1638) 1638\n",
      "(2490, 1638) 1638\n",
      "(2490, 994) 994\n",
      "(2490, 994) 994\n",
      "(2490, 766) 766\n",
      "(2490, 766) 766\n",
      "(2490, 487) 487\n",
      "(2490, 487) 487\n",
      "(2490, 439) 439\n",
      "(2490, 439) 439\n",
      "(2490, 404) 404\n",
      "(2490, 404) 404\n",
      "(2490, 527) 527\n",
      "(2490, 527) 527\n",
      "(2490, 4113) 4113\n",
      "(2490, 4113) 4113\n",
      "(2490, 892) 892\n",
      "(2490, 892) 892\n",
      "(2490, 1759) 1759\n",
      "(2490, 1759) 1759\n",
      "(2490, 1742) 1742\n",
      "(2490, 1742) 1742\n",
      "(2490, 1339) 1339\n",
      "(2490, 1339) 1339\n",
      "(2490, 1242) 1242\n",
      "(2490, 1242) 1242\n",
      "(2490, 589) 589\n",
      "(2490, 589) 589\n",
      "(2490, 1345) 1345\n",
      "(2490, 1345) 1345\n",
      "(2490, 1724) 1724\n",
      "(2490, 1724) 1724\n",
      "(2490, 1538) 1538\n",
      "(2490, 1538) 1538\n",
      "(2490, 1650) 1650\n",
      "(2490, 1650) 1650\n",
      "(2490, 992) 992\n",
      "(2490, 992) 992\n",
      "(2490, 756) 756\n",
      "(2490, 756) 756\n",
      "(2490, 491) 491\n",
      "(2490, 491) 491\n",
      "(2490, 439) 439\n",
      "(2490, 439) 439\n",
      "(2490, 404) 404\n",
      "(2490, 404) 404\n",
      "(2490, 555) 555\n",
      "(2490, 555) 555\n",
      "(2490, 4145) 4145\n",
      "(2490, 4145) 4145\n",
      "(2490, 1089) 1089\n",
      "(2490, 1089) 1089\n",
      "(2490, 2226) 2226\n",
      "(2490, 2226) 2226\n",
      "(2490, 2226) 2226\n",
      "(2490, 2226) 2226\n",
      "(2490, 1410) 1410\n",
      "(2490, 1410) 1410\n",
      "(2490, 1242) 1242\n",
      "(2490, 1242) 1242\n",
      "(2490, 881) 881\n",
      "(2490, 881) 881\n",
      "(2490, 1316) 1316\n",
      "(2490, 1316) 1316\n",
      "(2490, 1250) 1250\n",
      "(2490, 1250) 1250\n",
      "(2490, 1017) 1017\n",
      "(2490, 1017) 1017\n",
      "(2490, 1650) 1650\n",
      "(2490, 1650) 1650\n",
      "(2490, 994) 994\n",
      "(2490, 994) 994\n",
      "(2490, 766) 766\n",
      "(2490, 766) 766\n",
      "(2490, 491) 491\n",
      "(2490, 491) 491\n",
      "(2490, 439) 439\n",
      "(2490, 439) 439\n",
      "(2490, 404) 404\n",
      "(2490, 404) 404\n",
      "(2490, 557) 557\n",
      "(2490, 557) 557\n",
      "(2490, 4161) 4161\n",
      "(2490, 4161) 4161\n",
      "(2490, 1025) 1025\n",
      "(2490, 1025) 1025\n",
      "(2490, 1907) 1907\n",
      "(2490, 1907) 1907\n",
      "(2490, 1733) 1733\n",
      "(2490, 1733) 1733\n",
      "(2490, 1424) 1424\n",
      "(2490, 1424) 1424\n",
      "(2490, 1242) 1242\n",
      "(2490, 1242) 1242\n",
      "(2490, 1324) 1324\n",
      "(2490, 1324) 1324\n",
      "(2490, 1286) 1286\n",
      "(2490, 1286) 1286\n",
      "(2490, 1299) 1299\n",
      "(2490, 1299) 1299\n",
      "(2490, 1131) 1131\n",
      "(2490, 1131) 1131\n",
      "(2490, 1650) 1650\n",
      "(2490, 1650) 1650\n",
      "(2490, 992) 992\n",
      "(2490, 992) 992\n",
      "(2490, 764) 764\n",
      "(2490, 764) 764\n",
      "(2490, 491) 491\n",
      "(2490, 491) 491\n",
      "(2490, 439) 439\n",
      "(2490, 439) 439\n",
      "(2490, 404) 404\n",
      "(2490, 404) 404\n",
      "(2490, 585) 585\n",
      "(2490, 585) 585\n",
      "(2490, 4185) 4185\n",
      "(2490, 4185) 4185\n",
      "(2490, 798) 798\n",
      "(2490, 798) 798\n",
      "(2490, 1478) 1478\n",
      "(2490, 1478) 1478\n",
      "(2490, 1693) 1693\n",
      "(2490, 1693) 1693\n",
      "(2490, 902) 902\n",
      "(2490, 902) 902\n",
      "(2490, 853) 853\n",
      "(2490, 853) 853\n",
      "(2490, 443) 443\n",
      "(2490, 443) 443\n",
      "(2490, 1293) 1293\n",
      "(2490, 1293) 1293\n",
      "(2490, 1190) 1190\n",
      "(2490, 1190) 1190\n",
      "(2490, 844) 844\n",
      "(2490, 844) 844\n",
      "(2490, 1400) 1400\n",
      "(2490, 1400) 1400\n",
      "(2490, 917) 917\n",
      "(2490, 917) 917\n",
      "(2490, 742) 742\n",
      "(2490, 742) 742\n",
      "(2490, 487) 487\n",
      "(2490, 487) 487\n",
      "(2490, 439) 439\n",
      "(2490, 439) 439\n",
      "(2490, 404) 404\n",
      "(2490, 404) 404\n",
      "(2490, 573) 573\n",
      "(2490, 573) 573\n",
      "(2490, 3859) 3859\n",
      "(2490, 3859) 3859\n",
      "(2490, 923) 923\n",
      "(2490, 923) 923\n",
      "(2490, 1718) 1718\n",
      "(2490, 1718) 1718\n",
      "(2490, 1797) 1797\n",
      "(2490, 1797) 1797\n",
      "(2490, 1149) 1149\n",
      "(2490, 1149) 1149\n",
      "(2490, 1209) 1209\n",
      "(2490, 1209) 1209\n",
      "(2490, 685) 685\n",
      "(2490, 685) 685\n",
      "(2490, 1357) 1357\n",
      "(2490, 1357) 1357\n",
      "(2490, 1425) 1425\n",
      "(2490, 1425) 1425\n",
      "(2490, 1142) 1142\n",
      "(2490, 1142) 1142\n",
      "(2490, 1632) 1632\n",
      "(2490, 1632) 1632\n",
      "(2490, 994) 994\n",
      "(2490, 994) 994\n",
      "(2490, 764) 764\n",
      "(2490, 764) 764\n",
      "(2490, 491) 491\n",
      "(2490, 491) 491\n",
      "(2490, 439) 439\n",
      "(2490, 439) 439\n",
      "(2490, 404) 404\n",
      "(2490, 404) 404\n",
      "(2490, 575) 575\n",
      "(2490, 575) 575\n",
      "(2490, 4159) 4159\n",
      "(2490, 4159) 4159\n",
      "(2490, 1091) 1091\n",
      "(2490, 1091) 1091\n",
      "(2490, 2264) 2264\n",
      "(2490, 2264) 2264\n",
      "(2490, 2092) 2092\n",
      "(2490, 2092) 2092\n",
      "(2490, 1288) 1288\n",
      "(2490, 1288) 1288\n",
      "(2490, 1240) 1240\n",
      "(2490, 1240) 1240\n",
      "(2490, 743) 743\n",
      "(2490, 743) 743\n"
     ]
    }
   ],
   "source": [
    "# all model\n",
    "for epoch in range(3):\n",
    "    for subject in subjects:\n",
    "        \n",
    "        # load dataset (already merged across runs)\n",
    "        with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_merge_across_run.pkg'),'rb') as f:\n",
    "            dataset_merge = pickle.load(f)\n",
    "        \n",
    "        dataset_merge_new = []\n",
    "        \n",
    "        # let's merge further across runs and sessions\n",
    "        uniq_roi = order #['area4', 'v1', 'v2', 'ips', 'fef', 'sfg', 'mfg', 'ifg']\n",
    "        #list(set([dataset_merge[i].descriptors['roi'] for i in range(len(dataset_merge))]))\n",
    "        uniq_sess = sorted(list(set([dataset_merge[i].descriptors['session'] for i in range(len(dataset_merge))])))\n",
    "        \n",
    "        \n",
    "        for curr_roi in uniq_roi: # for each roi\n",
    "            data_across_sess = []\n",
    "            idx = 0\n",
    "            idx_run = 0  \n",
    "            res_across_sess = []\n",
    "            dof_task_across_sess = []\n",
    "            dof_all_across_sess = []\n",
    "            \n",
    "            for idx_sess,curr_sess in enumerate(uniq_sess):\n",
    "                #print(curr_sess)\n",
    "                data_temp = copy.deepcopy([x for x in dataset_merge if (x.descriptors['session'] == curr_sess) and\n",
    "                                                         (x.descriptors['roi'] == curr_roi)])\n",
    "                \n",
    "                # load corresponding residual map too\n",
    "                with open(os.path.join(subject,'res',curr_sess,'residual_'+curr_roi+'.pkg'),'rb') as f:\n",
    "                    residual_mat,num_res,dof_task,dof_all = pickle.load(f)  \n",
    "                    \n",
    "                res_across_sess.append(residual_mat)\n",
    "                dof_task_across_sess.append(dof_task)\n",
    "                dof_all_across_sess.append(dof_all)\n",
    "\n",
    "                # for each run combination\n",
    "                for rr in range(len(data_temp)):\n",
    "                    curr_run = data_temp[rr].descriptors['run']\n",
    "                    curr_conds = data_temp[rr].obs_descriptors['conds']\n",
    "                    curr_conds_index = data_temp[rr].obs_descriptors['conds_index']\n",
    "                    \n",
    "                    # update conditions and condition index\n",
    "                    # for the LR model\n",
    "                    # calculate new condition names and indices\n",
    "                    new_conds = curr_conds #curr_conds[x[-1]+'-'+curr_run+'-'+curr_sess for x in curr_conds]\n",
    "                    new_conds_index = [x + idx for x in curr_conds_index]\n",
    "                    pattern_index = curr_conds_index   \n",
    "                    #new_run_name = curr_run_name\n",
    "\n",
    "                    run_pair_index = [idx_run for _ in range(len(curr_conds))]\n",
    "                    idx_run += 1\n",
    "                    sess_index = [idx_sess for _ in range(len(curr_conds))]\n",
    "\n",
    "            #                     ecc_val = data_temp[rr].descriptors.pop('ecc', None)\n",
    "            #                     ecc_index = [ecc_val for _ in range(len(curr_conds))]\n",
    "\n",
    "                    idx = idx+len(np.unique(new_conds)) #uniq_sess.index(curr_sess)*2\n",
    "                    #print(new_conds)\n",
    "\n",
    "                    # update\n",
    "                    data_temp[rr].obs_descriptors['conds'] = new_conds\n",
    "                    data_temp[rr].obs_descriptors['conds_index'] = new_conds_index\n",
    "\n",
    "                    data_temp[rr].obs_descriptors['pattern_index'] = pattern_index\n",
    "                    data_temp[rr].obs_descriptors['run_pair_index'] = run_pair_index\n",
    "                    data_temp[rr].obs_descriptors['sess_index'] = sess_index\n",
    "            #                    data_temp[rr].obs_descriptors['ecc_index'] = sess_index\n",
    "\n",
    "                    # update the descriptors too\n",
    "                    data_temp[rr].descriptors.pop('session', None)\n",
    "                    data_temp[rr].descriptors.pop('run', None)\n",
    "                    data_temp[rr].descriptors.pop('ecc_or_set', None)\n",
    "                    data_temp[rr].descriptors['name'] = subject + ' | ' + curr_roi\n",
    "\n",
    "                data_across_sess.extend(data_temp)\n",
    "            #print(len(data_across_sess)) # 5 or 6 run combinations \n",
    "\n",
    "            #    now we can merge datasets across sessions\n",
    "            dataset_test = rsatoolbox.data.dataset.merge_subsets(data_across_sess)\n",
    "            #             print(len(dataset_test.obs_descriptors['conds']))\n",
    "\n",
    "            \n",
    "            # remove voxels that are nan\n",
    "            measure = dataset_test.get_measurements() # beta map\n",
    "            residual_measure = np.vstack(res_across_sess)# concatenate residual measures\n",
    "            vox_not_nan = (~np.isnan(measure).any(axis=0)) & (~np.isnan(residual_measure).any(axis=0)) \n",
    "            # any column (voxel/channel) that is nan for both beta map and residual map\n",
    "            \n",
    "            # update beta map\n",
    "            measure = measure[:,vox_not_nan]\n",
    "            nVox_not_nan = measure.shape[1] # number of voxels  \n",
    "            \n",
    "            # update channel descriptor and measurements\n",
    "            chn_des = {'voxels': np.array(['voxel_' + str(x) for x in np.arange(nVox_not_nan)])} # descriptors for channels\n",
    "            dataset_test.measurements = measure\n",
    "            dataset_test.channel_descriptors = chn_des\n",
    "            \n",
    "            # append to merged dataset\n",
    "            dataset_merge_new.append(dataset_test)  \n",
    "            \n",
    "            # update residual measurement for each session\n",
    "            for idx_sess,curr_sess in enumerate(uniq_sess):\n",
    "                residual_mat_subset = res_across_sess[idx_sess][:,vox_not_nan]\n",
    "                print(residual_mat_subset.shape,nVox_not_nan)\n",
    "                \n",
    "                # save residual dataset\n",
    "                with open(os.path.join(subject,'res',curr_sess,'all_subset_residual_'+curr_roi+'.pkg'),'wb') as f:\n",
    "                    pickle.dump([residual_mat_subset,dof_task_across_sess[idx_sess],dof_all_across_sess[idx_sess],],f)                \n",
    "\n",
    "            \n",
    "        # save dataset\n",
    "        with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_merge_across_sess_all.pkg'),'wb') as f:\n",
    "            pickle.dump(dataset_merge_new,f)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "696986c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 743)\n",
      "ifg-md\n",
      "(2490, 1984)\n"
     ]
    }
   ],
   "source": [
    "print(measure.shape)\n",
    "print(curr_roi)\n",
    "print(residual_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d34fea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "rsatoolbox.data.Dataset\n",
      "measurements = \n",
      "[[ 3.25829458  0.77345365  1.2984947  ...  2.53576016  9.57048893\n",
      "  12.50533104]\n",
      " [ 2.58594298  0.81358588  1.02677143 ... -0.74873906  7.44950104\n",
      "  12.72837448]\n",
      " [ 3.60658789  2.58145475  3.83818889 ...  2.80429077 10.73773861\n",
      "  15.77918148]\n",
      " [ 1.57926857 -1.21271455  0.19060683 ...  2.58357906  3.4319756\n",
      "   8.44166088]\n",
      " [ 2.92465663  2.1232121   1.40172887 ...  0.75550526  5.02144384\n",
      "   9.05018234]]\n",
      "...\n",
      "\n",
      "descriptors: \n",
      "roi = ips-wang15\n",
      "subj = f19\n",
      "name = f19 | ips-wang15\n",
      "\n",
      "\n",
      "obs_descriptors: \n",
      "pattern_index = [0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4\n",
      " 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1\n",
      " 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7]\n",
      "sess_index = [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "run_name = ['R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2'\n",
      " 'R2' 'R2' 'R4' 'R4' 'R4' 'R4' 'R4' 'R4' 'R4' 'R4' 'R3' 'R3' 'R3' 'R3'\n",
      " 'R3' 'R3' 'R3' 'R3' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5' 'R6' 'R6'\n",
      " 'R6' 'R6' 'R6' 'R6' 'R6' 'R6' 'R3' 'R3' 'R3' 'R3' 'R3' 'R3' 'R3' 'R3'\n",
      " 'R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5'\n",
      " 'R5' 'R5' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2' 'R6' 'R6' 'R6' 'R6'\n",
      " 'R6' 'R6' 'R6' 'R6' 'R4' 'R4' 'R4' 'R4' 'R4' 'R4' 'R4' 'R4']\n",
      "conds_index = [ 0  1  2  3  4  5  6  7  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\n",
      "  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39\n",
      " 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 40 41 42 43 44 45 46 47]\n",
      "run_pair_index = [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "conds = ['CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR'\n",
      " 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR'\n",
      " 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR'\n",
      " 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR'\n",
      " 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR'\n",
      " 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR'\n",
      " 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR'\n",
      " 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR']\n",
      "\n",
      "\n",
      "channel_descriptors: \n",
      "voxels = ['voxel_0' 'voxel_1' 'voxel_2' ... 'voxel_4156' 'voxel_4157' 'voxel_4158']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_merge_new))\n",
    "print(dataset_merge_new[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4db4703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "775e21ae",
   "metadata": {},
   "source": [
    "# Merge dataset for simpler model (LR, WT, ecc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c251f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to further merge the dataset, so that we have different sessions and runs in one matrix\n",
    "# for example, if we want to construct a WT (shape) matrix\n",
    "# We need to rearrange the conditions as follows:\n",
    "# W-Run14-S1, T-Run14-S1, W-R25-S1, T-R25-S1, W-R14-S2, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a07a433",
   "metadata": {},
   "source": [
    "## LR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5227b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = ['f09','f10','f11','f12','f15','f16','f17','f18','f19']\n",
    "epochs = ['delay','response','stimulus']\n",
    "exp2_subjects = ['f17','f18','f19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c052c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR model, averaged across eccentricities\n",
    "for epoch in range(3):\n",
    "    for subject in subjects:\n",
    "        \n",
    "        # load dataset (already merged across runs)\n",
    "        with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_merge_across_run.pkg'),'rb') as f:\n",
    "            dataset_merge = pickle.load(f)\n",
    "        \n",
    "        dataset_merge_new = []\n",
    "        \n",
    "        # let's merge further across runs and sessions\n",
    "        uniq_roi = order #['area4', 'v1', 'v2', 'ips', 'fef', 'sfg', 'mfg', 'ifg']\n",
    "        #list(set([dataset_merge[i].descriptors['roi'] for i in range(len(dataset_merge))]))\n",
    "        uniq_sess = sorted(list(set([dataset_merge[i].descriptors['session'] for i in range(len(dataset_merge))])))\n",
    "        for curr_roi in uniq_roi:\n",
    "            data_across_sess = []\n",
    "            idx = 0\n",
    "            idx_cond = 0\n",
    "            idx_run = 0\n",
    "            for idx_sess,curr_sess in enumerate(uniq_sess):\n",
    "                #print(curr_sess)\n",
    "                data_temp = copy.deepcopy([x for x in dataset_merge if (x.descriptors['session'] == curr_sess) and\n",
    "                                                         (x.descriptors['roi'] == curr_roi)])\n",
    "\n",
    "                # for each run combination\n",
    "                for rr in range(len(data_temp)):\n",
    "                    curr_run = data_temp[rr].descriptors['run']\n",
    "                    curr_conds = data_temp[rr].obs_descriptors['conds']\n",
    "                    curr_conds_index = data_temp[rr].obs_descriptors['conds_index']\n",
    "                    #curr_run_name = data_temp[rr].obs_descriptors['run_name']\n",
    "                    \n",
    "                    # update conditions and condition index\n",
    "                    # for the LR model\n",
    "                    # calculate new condition names and indices\n",
    "                    new_conds = [x[-1]+'-'+curr_run+'-'+curr_sess for x in curr_conds]\n",
    "                    new_conds_index = [idx if x == 'L'+'-'+curr_run+'-'+curr_sess \n",
    "                                       else idx+1 for x in new_conds]\n",
    "                    \n",
    "                    pattern_index = [idx_cond if x == 'L'+'-'+curr_run+'-'+curr_sess \n",
    "                                     else idx_cond+1 for x in new_conds]       \n",
    "                    run_index = [idx_run for _ in range(len(curr_conds))]\n",
    "                    idx_run += 1\n",
    "                    sess_index = [idx_sess for _ in range(len(curr_conds))]\n",
    "                    \n",
    "#                     ecc_val = data_temp[rr].descriptors.pop('ecc', None)\n",
    "#                     ecc_index = [ecc_val for _ in range(len(curr_conds))]\n",
    "            \n",
    "                    idx = idx+2 #uniq_sess.index(curr_sess)*2\n",
    "                    #print(new_conds)\n",
    "                    \n",
    "                    # update\n",
    "                    data_temp[rr].obs_descriptors['full_conds'] = curr_conds\n",
    "                    data_temp[rr].obs_descriptors['conds'] = new_conds\n",
    "                    data_temp[rr].obs_descriptors['conds_index'] = new_conds_index\n",
    "                    \n",
    "                    data_temp[rr].obs_descriptors['pattern_index'] = pattern_index\n",
    "                    data_temp[rr].obs_descriptors['run_pair_index'] = run_index\n",
    "                    data_temp[rr].obs_descriptors['sess_index'] = sess_index\n",
    "#                    data_temp[rr].obs_descriptors['ecc_index'] = sess_index\n",
    "                    \n",
    "                    # update the descriptors too\n",
    "                    data_temp[rr].descriptors.pop('session', None)\n",
    "                    data_temp[rr].descriptors.pop('run', None)\n",
    "                    data_temp[rr].descriptors.pop('ecc', None)\n",
    "                    data_temp[rr].descriptors['name'] = subject + ' | ' + curr_roi\n",
    "                \n",
    "                data_across_sess.extend(data_temp)\n",
    "            #print(len(data_across_sess)) # 5 or 6 run combinations \n",
    "\n",
    "        #    now we can merge datasets across sessions\n",
    "            dataset_test = rsatoolbox.data.dataset.merge_subsets(data_across_sess)\n",
    "#             print(len(dataset_test.obs_descriptors['conds']))\n",
    "#             print(dataset_test.obs_descriptors['conds'])\n",
    "\n",
    "            # remove voxels that are nan\n",
    "            measure = dataset_test.get_measurements()\n",
    "            vox_not_nan = ~np.isnan(measure).any(axis=0) # voxels\n",
    "            measure = measure[:,vox_not_nan]\n",
    "            nVox_not_nan = measure.shape[1] # number of voxels  \n",
    "            \n",
    "            # update channel descriptor and measurements\n",
    "            chn_des = {'voxels': np.array(['voxel_' + str(x) for x in np.arange(nVox_not_nan)])} # descriptors for channels\n",
    "            dataset_test.measurements = measure\n",
    "            dataset_test.channel_descriptors = chn_des\n",
    "            \n",
    "            dataset_merge_new.append(dataset_test)  \n",
    "            \n",
    "        # save dataset\n",
    "        with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_merge_across_sess_LR.pkg'),'wb') as f:\n",
    "            pickle.dump(dataset_merge_new,f)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fdee0b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rsatoolbox.data.Dataset\n",
      "measurements = \n",
      "[[ 1.13681543  1.38350403  3.93112111 ...  1.02807379  0.39928079\n",
      "   1.18702257]\n",
      " [-2.84869862 -4.16433764 -0.19244017 ...  1.57189214  0.55069697\n",
      "   0.33742237]\n",
      " [-3.5486486  -3.39419794  0.74993289 ...  2.44093609  1.17255092\n",
      "  -1.37427986]\n",
      " [-1.26268065 -6.55327177 -0.56980014 ... -0.48392674 -0.04410791\n",
      "  -1.77953053]\n",
      " [-1.64067352 -2.59643316 -1.58815801 ...  2.16497278  1.5415889\n",
      "  -1.1298852 ]]\n",
      "...\n",
      "\n",
      "descriptors: \n",
      "ecc_or_set = P1P2\n",
      "roi = area4-ju50\n",
      "subj = f19\n",
      "name = f19 | area4-ju50\n",
      "\n",
      "\n",
      "obs_descriptors: \n",
      "pattern_index = [0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0\n",
      " 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0\n",
      " 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1]\n",
      "full_conds = ['CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR'\n",
      " 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR'\n",
      " 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR'\n",
      " 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR'\n",
      " 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR'\n",
      " 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR'\n",
      " 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR'\n",
      " 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR']\n",
      "sess_index = [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "run_name = ['R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2'\n",
      " 'R2' 'R2' 'R4' 'R4' 'R4' 'R4' 'R4' 'R4' 'R4' 'R4' 'R3' 'R3' 'R3' 'R3'\n",
      " 'R3' 'R3' 'R3' 'R3' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5' 'R6' 'R6'\n",
      " 'R6' 'R6' 'R6' 'R6' 'R6' 'R6' 'R3' 'R3' 'R3' 'R3' 'R3' 'R3' 'R3' 'R3'\n",
      " 'R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5'\n",
      " 'R5' 'R5' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2' 'R6' 'R6' 'R6' 'R6'\n",
      " 'R6' 'R6' 'R6' 'R6' 'R4' 'R4' 'R4' 'R4' 'R4' 'R4' 'R4' 'R4']\n",
      "conds_index = [ 0  0  1  1  0  0  1  1  0  0  1  1  0  0  1  1  2  2  3  3  2  2  3  3\n",
      "  2  2  3  3  2  2  3  3  4  4  5  5  4  4  5  5  4  4  5  5  4  4  5  5\n",
      "  6  6  7  7  6  6  7  7  6  6  7  7  6  6  7  7  8  8  9  9  8  8  9  9\n",
      "  8  8  9  9  8  8  9  9 10 10 11 11 10 10 11 11 10 10 11 11 10 10 11 11]\n",
      "run_pair_index = [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "conds = ['L-R1R2-S5934' 'L-R1R2-S5934' 'R-R1R2-S5934' 'R-R1R2-S5934'\n",
      " 'L-R1R2-S5934' 'L-R1R2-S5934' 'R-R1R2-S5934' 'R-R1R2-S5934'\n",
      " 'L-R1R2-S5934' 'L-R1R2-S5934' 'R-R1R2-S5934' 'R-R1R2-S5934'\n",
      " 'L-R1R2-S5934' 'L-R1R2-S5934' 'R-R1R2-S5934' 'R-R1R2-S5934'\n",
      " 'L-R4R3-S5934' 'L-R4R3-S5934' 'R-R4R3-S5934' 'R-R4R3-S5934'\n",
      " 'L-R4R3-S5934' 'L-R4R3-S5934' 'R-R4R3-S5934' 'R-R4R3-S5934'\n",
      " 'L-R4R3-S5934' 'L-R4R3-S5934' 'R-R4R3-S5934' 'R-R4R3-S5934'\n",
      " 'L-R4R3-S5934' 'L-R4R3-S5934' 'R-R4R3-S5934' 'R-R4R3-S5934'\n",
      " 'L-R5R6-S5934' 'L-R5R6-S5934' 'R-R5R6-S5934' 'R-R5R6-S5934'\n",
      " 'L-R5R6-S5934' 'L-R5R6-S5934' 'R-R5R6-S5934' 'R-R5R6-S5934'\n",
      " 'L-R5R6-S5934' 'L-R5R6-S5934' 'R-R5R6-S5934' 'R-R5R6-S5934'\n",
      " 'L-R5R6-S5934' 'L-R5R6-S5934' 'R-R5R6-S5934' 'R-R5R6-S5934'\n",
      " 'L-R3R1-S5946' 'L-R3R1-S5946' 'R-R3R1-S5946' 'R-R3R1-S5946'\n",
      " 'L-R3R1-S5946' 'L-R3R1-S5946' 'R-R3R1-S5946' 'R-R3R1-S5946'\n",
      " 'L-R3R1-S5946' 'L-R3R1-S5946' 'R-R3R1-S5946' 'R-R3R1-S5946'\n",
      " 'L-R3R1-S5946' 'L-R3R1-S5946' 'R-R3R1-S5946' 'R-R3R1-S5946'\n",
      " 'L-R5R2-S5946' 'L-R5R2-S5946' 'R-R5R2-S5946' 'R-R5R2-S5946'\n",
      " 'L-R5R2-S5946' 'L-R5R2-S5946' 'R-R5R2-S5946' 'R-R5R2-S5946'\n",
      " 'L-R5R2-S5946' 'L-R5R2-S5946' 'R-R5R2-S5946' 'R-R5R2-S5946'\n",
      " 'L-R5R2-S5946' 'L-R5R2-S5946' 'R-R5R2-S5946' 'R-R5R2-S5946'\n",
      " 'L-R6R4-S5946' 'L-R6R4-S5946' 'R-R6R4-S5946' 'R-R6R4-S5946'\n",
      " 'L-R6R4-S5946' 'L-R6R4-S5946' 'R-R6R4-S5946' 'R-R6R4-S5946'\n",
      " 'L-R6R4-S5946' 'L-R6R4-S5946' 'R-R6R4-S5946' 'R-R6R4-S5946'\n",
      " 'L-R6R4-S5946' 'L-R6R4-S5946' 'R-R6R4-S5946' 'R-R6R4-S5946']\n",
      "\n",
      "\n",
      "channel_descriptors: \n",
      "voxels = ['voxel_0' 'voxel_1' 'voxel_2' ... 'voxel_1354' 'voxel_1355' 'voxel_1356']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset_merge_new[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c9aa17",
   "metadata": {},
   "source": [
    "## LR model - 3-dva eccentricity or spatial set 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9f9bb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = ['f09','f10','f11','f12','f15','f16','f17','f18','f19']\n",
    "epochs = ['delay','response','stimulus']\n",
    "exp2_subjects = ['f17','f18','f19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b73dda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(3):\n",
    "    for subject in subjects:\n",
    "        \n",
    "        # load dataset (already merged across runs)\n",
    "        with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_run_ecc.pkg'),'rb') as f:\n",
    "            dataset_merge = pickle.load(f)\n",
    "        #print(dataset_merge[0])\n",
    "        \n",
    "        dataset_merge_new = []\n",
    "        uniq_roi = order #['area4', 'v1', 'v2', 'ips', 'fef', 'sfg', 'mfg', 'ifg']\n",
    "        #list(set([dataset_merge[i].descriptors['roi'] for i in range(len(dataset_merge))]))\n",
    "        uniq_sess = sorted(list(set([dataset_merge[i].descriptors['session'] for i in range(len(dataset_merge))])))\n",
    "        for curr_roi in uniq_roi: \n",
    "            # for each run and session\n",
    "            idx = 0\n",
    "            data_across_sess = []\n",
    "            \n",
    "            idx_cond = 0\n",
    "            idx_run = 0\n",
    "            for idx_sess,curr_sess in enumerate(uniq_sess):\n",
    "                data_temp = copy.deepcopy([x for x in dataset_merge if (x.descriptors['session'] == curr_sess) and\n",
    "                                                 (x.descriptors['roi'] == curr_roi)])\n",
    "                #print(len(data_temp))\n",
    "                for rr in range(len(data_temp)):\n",
    "                    curr_ecc = data_temp[rr].descriptors['ecc_or_set']\n",
    "                    \n",
    "                    #print(curr_ecc)\n",
    "                    if (curr_ecc == 'E2') | (curr_ecc == 'P2'): # 5.5 dva or spatial set 2\n",
    "                        continue\n",
    "                    \n",
    "                    curr_run = data_temp[rr].descriptors['run']\n",
    "                    curr_conds = data_temp[rr].obs_descriptors['conds']\n",
    "                    curr_conds_index = data_temp[rr].obs_descriptors['conds_index']\n",
    "                    curr_run_name = data_temp[rr].obs_descriptors['run_name']\n",
    "                    \n",
    "                    curr_data = data_temp[rr].get_measurements()\n",
    "                    #print(curr_run,curr_ecc,curr_data.shape)\n",
    "\n",
    "                    # remove nan conditions\n",
    "                    # which rows (conditions) do not have all nan values (valid conditions)\n",
    "                    cond_not_nan = ~np.isnan(curr_data).all(axis=1)\n",
    "                    #print(cond_not_nan)\n",
    "\n",
    "                    # let's remove those conditions\n",
    "                    curr_data = curr_data[cond_not_nan,:]\n",
    "                    #print(curr_data.shape)\n",
    "\n",
    "                    # remove those invalid conditions from the conds and conds_index too\n",
    "                    curr_conds = [curr_conds[i] for i in range(len(cond_not_nan)) if cond_not_nan[i]]\n",
    "                    curr_conds_index = [curr_conds_index[i] for i in range(len(cond_not_nan)) if cond_not_nan[i]]\n",
    "                    #print(curr_conds,curr_conds_index)\n",
    "                    new_run_name = [curr_run_name[i] for i in range(len(cond_not_nan)) if cond_not_nan[i]]\n",
    "                    \n",
    "                    # update conditions and condition index\n",
    "                    # for the LR model\n",
    "                    # calculate new condition names and indices\n",
    "                    if subject in exp2_subjects:\n",
    "                        new_conds = [x[-2]+'-'+curr_run+'-'+curr_sess for x in curr_conds]\n",
    "                    else:\n",
    "                        new_conds = [x[-1]+'-'+curr_run+'-'+curr_sess for x in curr_conds]\n",
    "                        \n",
    "                    new_conds_index = [idx if x == 'L'+'-'+curr_run+'-'+curr_sess \n",
    "                                       else idx+1 for x in new_conds]\n",
    "                    idx = idx+2 #uniq_sess.index(curr_sess)*2\n",
    "                    #print(new_conds)\n",
    "                    \n",
    "                    pattern_index = [idx_cond if x == 'L'+'-'+curr_run+'-'+curr_sess \n",
    "                                     else idx_cond+1 for x in new_conds]       \n",
    "                    run_pair_index = [idx_run for _ in range(len(curr_conds))]\n",
    "                    sess_index = [idx_sess for _ in range(len(curr_conds))]\n",
    "                    idx_run += 1\n",
    "\n",
    "                    # update\n",
    "                    data_temp[rr].obs_descriptors['full_conds'] = [c[:3] for c in curr_conds]\n",
    "                    data_temp[rr].measurements = curr_data\n",
    "                    data_temp[rr].obs_descriptors['conds'] = new_conds\n",
    "                    data_temp[rr].obs_descriptors['conds_index'] = new_conds_index\n",
    "                    data_temp[rr].obs_descriptors['run_name'] = new_run_name\n",
    "                    data_temp[rr].obs_descriptors['pattern_index'] = pattern_index\n",
    "                    data_temp[rr].obs_descriptors['run_pair_index'] = run_pair_index\n",
    "                    data_temp[rr].obs_descriptors['sess_index'] = sess_index\n",
    "                    \n",
    "                    # update the descriptors too\n",
    "                    data_temp[rr].descriptors.pop('session', None)\n",
    "                    data_temp[rr].descriptors.pop('run', None)\n",
    "                    #data_temp[rr].descriptors.pop('ecc', None)\n",
    "                    data_temp[rr].descriptors['name'] = subject + ' | ' + curr_roi + ' | ' + curr_ecc\n",
    "                    \n",
    "                    \n",
    "                    #print(data_temp[rr])\n",
    "                    data_across_sess.extend([data_temp[rr]])\n",
    "                    #print(len(data_across_sess)) # 5 or 6 run combinations \n",
    "\n",
    "            #    now we can merge datasets across sessions\n",
    "            dataset_test = rsatoolbox.data.dataset.merge_subsets(data_across_sess)\n",
    "            #             print(len(dataset_test.obs_descriptors['conds']))\n",
    "            #             print(dataset_test.obs_descriptors['conds'])\n",
    "\n",
    "            # remove voxels that are nan\n",
    "            measure = dataset_test.get_measurements()\n",
    "            vox_not_nan = ~np.isnan(measure).any(axis=0) # voxels\n",
    "            measure = measure[:,vox_not_nan]\n",
    "            nVox_not_nan = measure.shape[1] # number of voxels  \n",
    "\n",
    "            # update channel descriptor and measurements\n",
    "            chn_des = {'voxels': np.array(['voxel_' + str(x) for x in np.arange(nVox_not_nan)])} # descriptors for channels\n",
    "            dataset_test.measurements = measure\n",
    "            dataset_test.channel_descriptors = chn_des\n",
    "\n",
    "            dataset_merge_new.append(dataset_test)  \n",
    "\n",
    "        # save dataset\n",
    "        if subject in exp2_subjects:\n",
    "            with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_merge_across_sess_LR_SP1.pkg'),'wb') as f:\n",
    "                pickle.dump(dataset_merge_new,f)\n",
    "        else:\n",
    "            with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_merge_across_sess_LR_3dva.pkg'),'wb') as f:\n",
    "                pickle.dump(dataset_merge_new,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71fd554d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rsatoolbox.data.Dataset\n",
      "measurements = \n",
      "[[ 1.13681543  1.38350403  3.93112111 ...  1.02807379  0.39928079\n",
      "   1.18702257]\n",
      " [-2.84869862 -4.16433764 -0.19244017 ...  1.57189214  0.55069697\n",
      "   0.33742237]\n",
      " [-3.5486486  -3.39419794  0.74993289 ...  2.44093609  1.17255092\n",
      "  -1.37427986]\n",
      " [-1.26268065 -6.55327177 -0.56980014 ... -0.48392674 -0.04410791\n",
      "  -1.77953053]\n",
      " [-1.64067352 -2.59643316 -1.58815801 ...  2.16497278  1.5415889\n",
      "  -1.1298852 ]]\n",
      "...\n",
      "\n",
      "descriptors: \n",
      "ecc_or_set = P1\n",
      "roi = area4-ju50\n",
      "subj = f19\n",
      "name = f19 | area4-ju50 | P1\n",
      "\n",
      "\n",
      "obs_descriptors: \n",
      "pattern_index = [0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0\n",
      " 0 1 1 0 0 1 1 0 0 1 1]\n",
      "full_conds = ['CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR'\n",
      " 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR'\n",
      " 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR'\n",
      " 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR']\n",
      "sess_index = [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1]\n",
      "run_name = ['R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R4' 'R4' 'R4' 'R4' 'R4' 'R4'\n",
      " 'R4' 'R4' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5' 'R3' 'R3' 'R3' 'R3'\n",
      " 'R3' 'R3' 'R3' 'R3' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5' 'R6' 'R6'\n",
      " 'R6' 'R6' 'R6' 'R6' 'R6' 'R6']\n",
      "conds_index = [ 0  0  1  1  0  0  1  1  2  2  3  3  2  2  3  3  4  4  5  5  4  4  5  5\n",
      "  6  6  7  7  6  6  7  7  8  8  9  9  8  8  9  9 10 10 11 11 10 10 11 11]\n",
      "run_pair_index = [0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 4 4 4 4 4\n",
      " 4 4 4 5 5 5 5 5 5 5 5]\n",
      "conds = ['L-R1-S5934' 'L-R1-S5934' 'R-R1-S5934' 'R-R1-S5934' 'L-R1-S5934'\n",
      " 'L-R1-S5934' 'R-R1-S5934' 'R-R1-S5934' 'L-R4-S5934' 'L-R4-S5934'\n",
      " 'R-R4-S5934' 'R-R4-S5934' 'L-R4-S5934' 'L-R4-S5934' 'R-R4-S5934'\n",
      " 'R-R4-S5934' 'L-R5-S5934' 'L-R5-S5934' 'R-R5-S5934' 'R-R5-S5934'\n",
      " 'L-R5-S5934' 'L-R5-S5934' 'R-R5-S5934' 'R-R5-S5934' 'L-R3-S5946'\n",
      " 'L-R3-S5946' 'R-R3-S5946' 'R-R3-S5946' 'L-R3-S5946' 'L-R3-S5946'\n",
      " 'R-R3-S5946' 'R-R3-S5946' 'L-R5-S5946' 'L-R5-S5946' 'R-R5-S5946'\n",
      " 'R-R5-S5946' 'L-R5-S5946' 'L-R5-S5946' 'R-R5-S5946' 'R-R5-S5946'\n",
      " 'L-R6-S5946' 'L-R6-S5946' 'R-R6-S5946' 'R-R6-S5946' 'L-R6-S5946'\n",
      " 'L-R6-S5946' 'R-R6-S5946' 'R-R6-S5946']\n",
      "\n",
      "\n",
      "channel_descriptors: \n",
      "voxels = ['voxel_0' 'voxel_1' 'voxel_2' ... 'voxel_1354' 'voxel_1355' 'voxel_1356']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset_merge_new[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a05109",
   "metadata": {},
   "source": [
    "## LR model - 5.5-dva eccentricity or spatial set 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "012c446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(3):\n",
    "    for subject in subjects:\n",
    "        \n",
    "        # load dataset (already merged across runs)\n",
    "        with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_run_ecc.pkg'),'rb') as f:\n",
    "            dataset_merge = pickle.load(f)\n",
    "        #print(dataset_merge[0])\n",
    "        \n",
    "        dataset_merge_new = []\n",
    "        uniq_roi = order #['area4', 'v1', 'v2', 'ips', 'fef', 'sfg', 'mfg', 'ifg']\n",
    "        #list(set([dataset_merge[i].descriptors['roi'] for i in range(len(dataset_merge))]))\n",
    "        uniq_sess = sorted(list(set([dataset_merge[i].descriptors['session'] for i in range(len(dataset_merge))])))\n",
    "        for curr_roi in uniq_roi: \n",
    "            # for each run and session\n",
    "            idx = 0\n",
    "            data_across_sess = []\n",
    "            \n",
    "            idx_cond = 0\n",
    "            idx_run = 0\n",
    "            for idx_sess,curr_sess in enumerate(uniq_sess):\n",
    "                data_temp = copy.deepcopy([x for x in dataset_merge if (x.descriptors['session'] == curr_sess) and\n",
    "                                                 (x.descriptors['roi'] == curr_roi)])\n",
    "                #print(len(data_temp))\n",
    "                for rr in range(len(data_temp)):\n",
    "                    curr_ecc = data_temp[rr].descriptors['ecc_or_set']\n",
    "                    \n",
    "                    #print(curr_ecc)\n",
    "                    if (curr_ecc == 'E1') | (curr_ecc == 'P1'): # 3 dva or spatial set 1\n",
    "                        continue\n",
    "                    \n",
    "                    curr_run = data_temp[rr].descriptors['run']\n",
    "                    curr_conds = data_temp[rr].obs_descriptors['conds']\n",
    "                    curr_conds_index = data_temp[rr].obs_descriptors['conds_index']\n",
    "                    curr_run_name = data_temp[rr].obs_descriptors['run_name']\n",
    "                    \n",
    "                    curr_data = data_temp[rr].get_measurements()\n",
    "                    #print(curr_run,curr_ecc,curr_data.shape)\n",
    "\n",
    "                    # remove nan conditions\n",
    "                    # which rows (conditions) do not have all nan values (valid conditions)\n",
    "                    cond_not_nan = ~np.isnan(curr_data).all(axis=1)\n",
    "                    #print(cond_not_nan)\n",
    "\n",
    "                    # let's remove those conditions\n",
    "                    curr_data = curr_data[cond_not_nan,:]\n",
    "                    #print(curr_data.shape)\n",
    "\n",
    "                    # remove those invalid conditions from the conds and conds_index too\n",
    "                    curr_conds = [curr_conds[i] for i in range(len(cond_not_nan)) if cond_not_nan[i]]\n",
    "                    curr_conds_index = [curr_conds_index[i] for i in range(len(cond_not_nan)) if cond_not_nan[i]]\n",
    "                    #print(curr_conds,curr_conds_index)\n",
    "                    new_run_name = [curr_run_name[i] for i in range(len(cond_not_nan)) if cond_not_nan[i]]\n",
    "                    \n",
    "                    # update conditions and condition index\n",
    "                    # for the LR model\n",
    "                    # calculate new condition names and indices\n",
    "                    if subject in exp2_subjects:\n",
    "                        new_conds = [x[-2]+'-'+curr_run+'-'+curr_sess for x in curr_conds]\n",
    "                    else:\n",
    "                        new_conds = [x[-1]+'-'+curr_run+'-'+curr_sess for x in curr_conds]\n",
    "                        \n",
    "                    new_conds_index = [idx if x == 'L'+'-'+curr_run+'-'+curr_sess \n",
    "                                       else idx+1 for x in new_conds]\n",
    "                    idx = idx+2 #uniq_sess.index(curr_sess)*2\n",
    "                    #print(new_conds)\n",
    "                    \n",
    "                    pattern_index = [idx_cond if x == 'L'+'-'+curr_run+'-'+curr_sess \n",
    "                                     else idx_cond+1 for x in new_conds]       \n",
    "                    run_pair_index = [idx_run for _ in range(len(curr_conds))]\n",
    "                    sess_index = [idx_sess for _ in range(len(curr_conds))]\n",
    "                    idx_run += 1\n",
    "\n",
    "                    # update\n",
    "                    data_temp[rr].obs_descriptors['full_conds'] = [c[:3] for c in curr_conds]\n",
    "                    data_temp[rr].measurements = curr_data\n",
    "                    data_temp[rr].obs_descriptors['conds'] = new_conds\n",
    "                    data_temp[rr].obs_descriptors['conds_index'] = new_conds_index\n",
    "                    data_temp[rr].obs_descriptors['run_name'] = new_run_name\n",
    "                    data_temp[rr].obs_descriptors['pattern_index'] = pattern_index\n",
    "                    data_temp[rr].obs_descriptors['run_pair_index'] = run_pair_index\n",
    "                    data_temp[rr].obs_descriptors['sess_index'] = sess_index\n",
    "                    \n",
    "                    # update the descriptors too\n",
    "                    data_temp[rr].descriptors.pop('session', None)\n",
    "                    data_temp[rr].descriptors.pop('run', None)\n",
    "                    #data_temp[rr].descriptors.pop('ecc', None)\n",
    "                    data_temp[rr].descriptors['name'] = subject + ' | ' + curr_roi + ' | ' + curr_ecc\n",
    "                    \n",
    "                    \n",
    "                    #print(data_temp[rr])\n",
    "                    data_across_sess.extend([data_temp[rr]])\n",
    "                    #print(len(data_across_sess)) # 5 or 6 run combinations \n",
    "\n",
    "            #    now we can merge datasets across sessions\n",
    "            dataset_test = rsatoolbox.data.dataset.merge_subsets(data_across_sess)\n",
    "            #             print(len(dataset_test.obs_descriptors['conds']))\n",
    "            #             print(dataset_test.obs_descriptors['conds'])\n",
    "\n",
    "            # remove voxels that are nan\n",
    "            measure = dataset_test.get_measurements()\n",
    "            vox_not_nan = ~np.isnan(measure).any(axis=0) # voxels\n",
    "            measure = measure[:,vox_not_nan]\n",
    "            nVox_not_nan = measure.shape[1] # number of voxels  \n",
    "\n",
    "            # update channel descriptor and measurements\n",
    "            chn_des = {'voxels': np.array(['voxel_' + str(x) for x in np.arange(nVox_not_nan)])} # descriptors for channels\n",
    "            dataset_test.measurements = measure\n",
    "            dataset_test.channel_descriptors = chn_des\n",
    "\n",
    "            dataset_merge_new.append(dataset_test)  \n",
    "\n",
    "        # save dataset\n",
    "        if subject in exp2_subjects:\n",
    "            with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_merge_across_sess_LR_SP2.pkg'),'wb') as f:\n",
    "                pickle.dump(dataset_merge_new,f)\n",
    "        else:\n",
    "            with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_merge_across_sess_LR_5.5dva.pkg'),'wb') as f:\n",
    "                pickle.dump(dataset_merge_new,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eceddc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rsatoolbox.data.Dataset\n",
      "measurements = \n",
      "[[-4.32295465 -6.16556215  0.7349382  ... -1.17331648  0.45550582\n",
      "  -2.78499293]\n",
      " [-1.72085452 -3.63415432 -1.65540874 ...  0.33521384 -0.54659218\n",
      "  -0.26850107]\n",
      " [-3.02165127 -6.29844046 -1.63729489 ...  0.08400786  1.30360365\n",
      "  -1.44681382]\n",
      " [-3.38832998 -6.20992613  0.13150956 ...  0.90429479 -2.20506287\n",
      "  -4.67829561]\n",
      " [-3.95717478 -6.65573072 -0.20510076 ... -3.87506557 -1.18707049\n",
      "  -2.55028176]]\n",
      "...\n",
      "\n",
      "descriptors: \n",
      "ecc_or_set = P2\n",
      "roi = area4-ju50\n",
      "subj = f19\n",
      "name = f19 | area4-ju50 | P2\n",
      "\n",
      "\n",
      "obs_descriptors: \n",
      "pattern_index = [0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0\n",
      " 0 1 1 0 0 1 1 0 0 1 1]\n",
      "full_conds = ['CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR'\n",
      " 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR'\n",
      " 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR'\n",
      " 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR']\n",
      "sess_index = [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1]\n",
      "run_name = ['R2' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2' 'R3' 'R3' 'R3' 'R3' 'R3' 'R3'\n",
      " 'R3' 'R3' 'R6' 'R6' 'R6' 'R6' 'R6' 'R6' 'R6' 'R6' 'R1' 'R1' 'R1' 'R1'\n",
      " 'R1' 'R1' 'R1' 'R1' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2' 'R4' 'R4'\n",
      " 'R4' 'R4' 'R4' 'R4' 'R4' 'R4']\n",
      "conds_index = [ 0  0  1  1  0  0  1  1  2  2  3  3  2  2  3  3  4  4  5  5  4  4  5  5\n",
      "  6  6  7  7  6  6  7  7  8  8  9  9  8  8  9  9 10 10 11 11 10 10 11 11]\n",
      "run_pair_index = [0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 4 4 4 4 4\n",
      " 4 4 4 5 5 5 5 5 5 5 5]\n",
      "conds = ['L-R2-S5934' 'L-R2-S5934' 'R-R2-S5934' 'R-R2-S5934' 'L-R2-S5934'\n",
      " 'L-R2-S5934' 'R-R2-S5934' 'R-R2-S5934' 'L-R3-S5934' 'L-R3-S5934'\n",
      " 'R-R3-S5934' 'R-R3-S5934' 'L-R3-S5934' 'L-R3-S5934' 'R-R3-S5934'\n",
      " 'R-R3-S5934' 'L-R6-S5934' 'L-R6-S5934' 'R-R6-S5934' 'R-R6-S5934'\n",
      " 'L-R6-S5934' 'L-R6-S5934' 'R-R6-S5934' 'R-R6-S5934' 'L-R1-S5946'\n",
      " 'L-R1-S5946' 'R-R1-S5946' 'R-R1-S5946' 'L-R1-S5946' 'L-R1-S5946'\n",
      " 'R-R1-S5946' 'R-R1-S5946' 'L-R2-S5946' 'L-R2-S5946' 'R-R2-S5946'\n",
      " 'R-R2-S5946' 'L-R2-S5946' 'L-R2-S5946' 'R-R2-S5946' 'R-R2-S5946'\n",
      " 'L-R4-S5946' 'L-R4-S5946' 'R-R4-S5946' 'R-R4-S5946' 'L-R4-S5946'\n",
      " 'L-R4-S5946' 'R-R4-S5946' 'R-R4-S5946']\n",
      "\n",
      "\n",
      "channel_descriptors: \n",
      "voxels = ['voxel_0' 'voxel_1' 'voxel_2' ... 'voxel_1354' 'voxel_1355' 'voxel_1356']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset_merge_new[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d7b9b1",
   "metadata": {},
   "source": [
    "## WT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e9ae08db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(3):\n",
    "    for subject in subjects:\n",
    "        \n",
    "        # load dataset (already merged across runs)\n",
    "        with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_merge_across_run.pkg'),'rb') as f:\n",
    "            dataset_merge = pickle.load(f)\n",
    "        \n",
    "        dataset_merge_new = []\n",
    "        \n",
    "        # let's merge further across runs and sessions\n",
    "        uniq_roi = order #['area4', 'v1', 'v2', 'ips', 'fef', 'sfg', 'mfg', 'ifg']\n",
    "        #list(set([dataset_merge[i].descriptors['roi'] for i in range(len(dataset_merge))]))\n",
    "        uniq_sess = sorted(list(set([dataset_merge[i].descriptors['session'] for i in range(len(dataset_merge))])))\n",
    "        for curr_roi in uniq_roi:\n",
    "            data_across_sess = []\n",
    "            idx = 0\n",
    "            idx_cond = 0\n",
    "            idx_run = 0\n",
    "            for idx_sess,curr_sess in enumerate(uniq_sess):\n",
    "                #print(curr_sess)\n",
    "                data_temp = copy.deepcopy([x for x in dataset_merge if (x.descriptors['session'] == curr_sess) and\n",
    "                                                         (x.descriptors['roi'] == curr_roi)])\n",
    "\n",
    "                # for each run combination\n",
    "                for rr in range(len(data_temp)):\n",
    "                    curr_run = data_temp[rr].descriptors['run']\n",
    "                    curr_conds = data_temp[rr].obs_descriptors['conds']\n",
    "                    curr_conds_index = data_temp[rr].obs_descriptors['conds_index']\n",
    "                    #curr_run_name = data_temp[rr].obs_descriptors['run_name']\n",
    "                    \n",
    "                    # update conditions and condition index\n",
    "                    # for the LR model\n",
    "                    # calculate new condition names and indices\n",
    "                    new_conds = [x[-2]+'-'+curr_run+'-'+curr_sess for x in curr_conds]\n",
    "                    new_conds_index = [idx if x == 'W'+'-'+curr_run+'-'+curr_sess \n",
    "                                       else idx+1 for x in new_conds]\n",
    "                    \n",
    "                    pattern_index = [idx_cond if x == 'W'+'-'+curr_run+'-'+curr_sess \n",
    "                                     else idx_cond+1 for x in new_conds]       \n",
    "                    run_index = [idx_run for _ in range(len(curr_conds))]\n",
    "                    idx_run += 1\n",
    "                    sess_index = [idx_sess for _ in range(len(curr_conds))]\n",
    "                    \n",
    "#                     ecc_val = data_temp[rr].descriptors.pop('ecc', None)\n",
    "#                     ecc_index = [ecc_val for _ in range(len(curr_conds))]\n",
    "            \n",
    "                    idx = idx+2 #uniq_sess.index(curr_sess)*2\n",
    "                    #print(new_conds)\n",
    "                    \n",
    "                    # update\n",
    "                    data_temp[rr].obs_descriptors['full_conds'] = curr_conds\n",
    "                    data_temp[rr].obs_descriptors['conds'] = new_conds\n",
    "                    data_temp[rr].obs_descriptors['conds_index'] = new_conds_index\n",
    "                    \n",
    "                    data_temp[rr].obs_descriptors['pattern_index'] = pattern_index\n",
    "                    data_temp[rr].obs_descriptors['run_pair_index'] = run_index\n",
    "                    data_temp[rr].obs_descriptors['sess_index'] = sess_index\n",
    "#                    data_temp[rr].obs_descriptors['ecc_index'] = sess_index\n",
    "                    \n",
    "                    # update the descriptors too\n",
    "                    data_temp[rr].descriptors.pop('session', None)\n",
    "                    data_temp[rr].descriptors.pop('run', None)\n",
    "                    data_temp[rr].descriptors.pop('ecc', None)\n",
    "                    data_temp[rr].descriptors['name'] = subject + ' | ' + curr_roi\n",
    "                \n",
    "                data_across_sess.extend(data_temp)\n",
    "            #print(len(data_across_sess)) # 5 or 6 run combinations \n",
    "\n",
    "        #    now we can merge datasets across sessions\n",
    "            dataset_test = rsatoolbox.data.dataset.merge_subsets(data_across_sess)\n",
    "#             print(len(dataset_test.obs_descriptors['conds']))\n",
    "#             print(dataset_test.obs_descriptors['conds'])\n",
    "\n",
    "            # remove voxels that are nan\n",
    "            measure = dataset_test.get_measurements()\n",
    "            vox_not_nan = ~np.isnan(measure).any(axis=0) # voxels\n",
    "            measure = measure[:,vox_not_nan]\n",
    "            nVox_not_nan = measure.shape[1] # number of voxels  \n",
    "            \n",
    "            # update channel descriptor and measurements\n",
    "            chn_des = {'voxels': np.array(['voxel_' + str(x) for x in np.arange(nVox_not_nan)])} # descriptors for channels\n",
    "            dataset_test.measurements = measure\n",
    "            dataset_test.channel_descriptors = chn_des\n",
    "            \n",
    "            dataset_merge_new.append(dataset_test)  \n",
    "            \n",
    "        # save dataset\n",
    "        with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_merge_across_sess_WT.pkg'),'wb') as f:\n",
    "            pickle.dump(dataset_merge_new,f)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8a902441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rsatoolbox.data.Dataset\n",
      "measurements = \n",
      "[[ 1.13681543  1.38350403  3.93112111 ...  1.02807379  0.39928079\n",
      "   1.18702257]\n",
      " [-2.84869862 -4.16433764 -0.19244017 ...  1.57189214  0.55069697\n",
      "   0.33742237]\n",
      " [-3.5486486  -3.39419794  0.74993289 ...  2.44093609  1.17255092\n",
      "  -1.37427986]\n",
      " [-1.26268065 -6.55327177 -0.56980014 ... -0.48392674 -0.04410791\n",
      "  -1.77953053]\n",
      " [-1.64067352 -2.59643316 -1.58815801 ...  2.16497278  1.5415889\n",
      "  -1.1298852 ]]\n",
      "...\n",
      "\n",
      "descriptors: \n",
      "ecc_or_set = P1P2\n",
      "roi = area4-ju50\n",
      "subj = f19\n",
      "name = f19 | area4-ju50\n",
      "\n",
      "\n",
      "obs_descriptors: \n",
      "pattern_index = [0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0\n",
      " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1\n",
      " 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1]\n",
      "full_conds = ['CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR'\n",
      " 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR'\n",
      " 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR'\n",
      " 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR'\n",
      " 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR'\n",
      " 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR'\n",
      " 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR'\n",
      " 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR']\n",
      "sess_index = [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "run_name = ['R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2'\n",
      " 'R2' 'R2' 'R4' 'R4' 'R4' 'R4' 'R4' 'R4' 'R4' 'R4' 'R3' 'R3' 'R3' 'R3'\n",
      " 'R3' 'R3' 'R3' 'R3' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5' 'R6' 'R6'\n",
      " 'R6' 'R6' 'R6' 'R6' 'R6' 'R6' 'R3' 'R3' 'R3' 'R3' 'R3' 'R3' 'R3' 'R3'\n",
      " 'R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5'\n",
      " 'R5' 'R5' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2' 'R6' 'R6' 'R6' 'R6'\n",
      " 'R6' 'R6' 'R6' 'R6' 'R4' 'R4' 'R4' 'R4' 'R4' 'R4' 'R4' 'R4']\n",
      "conds_index = [ 0  1  0  1  0  1  0  1  0  1  0  1  0  1  0  1  2  3  2  3  2  3  2  3\n",
      "  2  3  2  3  2  3  2  3  4  5  4  5  4  5  4  5  4  5  4  5  4  5  4  5\n",
      "  6  7  6  7  6  7  6  7  6  7  6  7  6  7  6  7  8  9  8  9  8  9  8  9\n",
      "  8  9  8  9  8  9  8  9 10 11 10 11 10 11 10 11 10 11 10 11 10 11 10 11]\n",
      "run_pair_index = [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "conds = ['W-R1R2-S5934' 'T-R1R2-S5934' 'W-R1R2-S5934' 'T-R1R2-S5934'\n",
      " 'W-R1R2-S5934' 'T-R1R2-S5934' 'W-R1R2-S5934' 'T-R1R2-S5934'\n",
      " 'W-R1R2-S5934' 'T-R1R2-S5934' 'W-R1R2-S5934' 'T-R1R2-S5934'\n",
      " 'W-R1R2-S5934' 'T-R1R2-S5934' 'W-R1R2-S5934' 'T-R1R2-S5934'\n",
      " 'W-R4R3-S5934' 'T-R4R3-S5934' 'W-R4R3-S5934' 'T-R4R3-S5934'\n",
      " 'W-R4R3-S5934' 'T-R4R3-S5934' 'W-R4R3-S5934' 'T-R4R3-S5934'\n",
      " 'W-R4R3-S5934' 'T-R4R3-S5934' 'W-R4R3-S5934' 'T-R4R3-S5934'\n",
      " 'W-R4R3-S5934' 'T-R4R3-S5934' 'W-R4R3-S5934' 'T-R4R3-S5934'\n",
      " 'W-R5R6-S5934' 'T-R5R6-S5934' 'W-R5R6-S5934' 'T-R5R6-S5934'\n",
      " 'W-R5R6-S5934' 'T-R5R6-S5934' 'W-R5R6-S5934' 'T-R5R6-S5934'\n",
      " 'W-R5R6-S5934' 'T-R5R6-S5934' 'W-R5R6-S5934' 'T-R5R6-S5934'\n",
      " 'W-R5R6-S5934' 'T-R5R6-S5934' 'W-R5R6-S5934' 'T-R5R6-S5934'\n",
      " 'W-R3R1-S5946' 'T-R3R1-S5946' 'W-R3R1-S5946' 'T-R3R1-S5946'\n",
      " 'W-R3R1-S5946' 'T-R3R1-S5946' 'W-R3R1-S5946' 'T-R3R1-S5946'\n",
      " 'W-R3R1-S5946' 'T-R3R1-S5946' 'W-R3R1-S5946' 'T-R3R1-S5946'\n",
      " 'W-R3R1-S5946' 'T-R3R1-S5946' 'W-R3R1-S5946' 'T-R3R1-S5946'\n",
      " 'W-R5R2-S5946' 'T-R5R2-S5946' 'W-R5R2-S5946' 'T-R5R2-S5946'\n",
      " 'W-R5R2-S5946' 'T-R5R2-S5946' 'W-R5R2-S5946' 'T-R5R2-S5946'\n",
      " 'W-R5R2-S5946' 'T-R5R2-S5946' 'W-R5R2-S5946' 'T-R5R2-S5946'\n",
      " 'W-R5R2-S5946' 'T-R5R2-S5946' 'W-R5R2-S5946' 'T-R5R2-S5946'\n",
      " 'W-R6R4-S5946' 'T-R6R4-S5946' 'W-R6R4-S5946' 'T-R6R4-S5946'\n",
      " 'W-R6R4-S5946' 'T-R6R4-S5946' 'W-R6R4-S5946' 'T-R6R4-S5946'\n",
      " 'W-R6R4-S5946' 'T-R6R4-S5946' 'W-R6R4-S5946' 'T-R6R4-S5946'\n",
      " 'W-R6R4-S5946' 'T-R6R4-S5946' 'W-R6R4-S5946' 'T-R6R4-S5946']\n",
      "\n",
      "\n",
      "channel_descriptors: \n",
      "voxels = ['voxel_0' 'voxel_1' 'voxel_2' ... 'voxel_1354' 'voxel_1355' 'voxel_1356']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset_merge_new[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939cacd6",
   "metadata": {},
   "source": [
    "## WT model - 3 dva or spatial set 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b4855b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(3):\n",
    "    for subject in subjects:\n",
    "        \n",
    "        # load dataset (already merged across runs)\n",
    "        with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_run_ecc.pkg'),'rb') as f:\n",
    "            dataset_merge = pickle.load(f)\n",
    "        #print(dataset_merge[0])\n",
    "        \n",
    "        dataset_merge_new = []\n",
    "        uniq_roi = order #['area4', 'v1', 'v2', 'ips', 'fef', 'sfg', 'mfg', 'ifg']\n",
    "        #list(set([dataset_merge[i].descriptors['roi'] for i in range(len(dataset_merge))]))\n",
    "        uniq_sess = sorted(list(set([dataset_merge[i].descriptors['session'] for i in range(len(dataset_merge))])))\n",
    "        for curr_roi in uniq_roi: \n",
    "            # for each run and session\n",
    "            idx = 0\n",
    "            data_across_sess = []\n",
    "            \n",
    "            idx_cond = 0\n",
    "            idx_run = 0\n",
    "            for idx_sess,curr_sess in enumerate(uniq_sess):\n",
    "                data_temp = copy.deepcopy([x for x in dataset_merge if (x.descriptors['session'] == curr_sess) and\n",
    "                                                 (x.descriptors['roi'] == curr_roi)])\n",
    "                #print(len(data_temp))\n",
    "                for rr in range(len(data_temp)):\n",
    "                    curr_ecc = data_temp[rr].descriptors['ecc_or_set']\n",
    "                    \n",
    "                    #print(curr_ecc)\n",
    "                    if (curr_ecc == 'E2') | (curr_ecc == 'P2'): # 5.5 dva or spatial set 2\n",
    "                        continue\n",
    "                    \n",
    "                    curr_run = data_temp[rr].descriptors['run']\n",
    "                    curr_conds = data_temp[rr].obs_descriptors['conds']\n",
    "                    curr_conds_index = data_temp[rr].obs_descriptors['conds_index']\n",
    "                    curr_run_name = data_temp[rr].obs_descriptors['run_name']\n",
    "                    \n",
    "                    curr_data = data_temp[rr].get_measurements()\n",
    "                    #print(curr_run,curr_ecc,curr_data.shape)\n",
    "\n",
    "                    # remove nan conditions\n",
    "                    # which rows (conditions) do not have all nan values (valid conditions)\n",
    "                    cond_not_nan = ~np.isnan(curr_data).all(axis=1)\n",
    "                    #print(cond_not_nan)\n",
    "\n",
    "                    # let's remove those conditions\n",
    "                    curr_data = curr_data[cond_not_nan,:]\n",
    "                    #print(curr_data.shape)\n",
    "\n",
    "                    # remove those invalid conditions from the conds and conds_index too\n",
    "                    curr_conds = [curr_conds[i] for i in range(len(cond_not_nan)) if cond_not_nan[i]]\n",
    "                    curr_conds_index = [curr_conds_index[i] for i in range(len(cond_not_nan)) if cond_not_nan[i]]\n",
    "                    #print(curr_conds,curr_conds_index)\n",
    "                    new_run_name = [curr_run_name[i] for i in range(len(cond_not_nan)) if cond_not_nan[i]]\n",
    "                    \n",
    "                    # update conditions and condition index\n",
    "                    # for the LR model\n",
    "                    # calculate new condition names and indices\n",
    "                    if subject in exp2_subjects:\n",
    "                        new_conds = [x[-3]+'-'+curr_run+'-'+curr_sess for x in curr_conds]\n",
    "                    else:\n",
    "                        new_conds = [x[-2]+'-'+curr_run+'-'+curr_sess for x in curr_conds]\n",
    "                        \n",
    "                    new_conds_index = [idx if x == 'W'+'-'+curr_run+'-'+curr_sess \n",
    "                                       else idx+1 for x in new_conds]\n",
    "                    idx = idx+2 #uniq_sess.index(curr_sess)*2\n",
    "                    #print(new_conds)\n",
    "                    \n",
    "                    pattern_index = [idx_cond if x == 'W'+'-'+curr_run+'-'+curr_sess \n",
    "                                     else idx_cond+1 for x in new_conds]       \n",
    "                    run_pair_index = [idx_run for _ in range(len(curr_conds))]\n",
    "                    sess_index = [idx_sess for _ in range(len(curr_conds))]\n",
    "                    idx_run += 1\n",
    "\n",
    "                    # update\n",
    "                    data_temp[rr].obs_descriptors['full_conds'] = [c[:3] for c in curr_conds]\n",
    "                    data_temp[rr].measurements = curr_data\n",
    "                    data_temp[rr].obs_descriptors['conds'] = new_conds\n",
    "                    data_temp[rr].obs_descriptors['conds_index'] = new_conds_index\n",
    "                    data_temp[rr].obs_descriptors['run_name'] = new_run_name\n",
    "                    data_temp[rr].obs_descriptors['pattern_index'] = pattern_index\n",
    "                    data_temp[rr].obs_descriptors['run_pair_index'] = run_pair_index\n",
    "                    data_temp[rr].obs_descriptors['sess_index'] = sess_index\n",
    "                    \n",
    "                    # update the descriptors too\n",
    "                    data_temp[rr].descriptors.pop('session', None)\n",
    "                    data_temp[rr].descriptors.pop('run', None)\n",
    "                    #data_temp[rr].descriptors.pop('ecc', None)\n",
    "                    data_temp[rr].descriptors['name'] = subject + ' | ' + curr_roi + ' | ' + curr_ecc\n",
    "                    \n",
    "                    \n",
    "                    #print(data_temp[rr])\n",
    "                    data_across_sess.extend([data_temp[rr]])\n",
    "                    #print(len(data_across_sess)) # 5 or 6 run combinations \n",
    "\n",
    "            #    now we can merge datasets across sessions\n",
    "            dataset_test = rsatoolbox.data.dataset.merge_subsets(data_across_sess)\n",
    "            #             print(len(dataset_test.obs_descriptors['conds']))\n",
    "            #             print(dataset_test.obs_descriptors['conds'])\n",
    "\n",
    "            # remove voxels that are nan\n",
    "            measure = dataset_test.get_measurements()\n",
    "            vox_not_nan = ~np.isnan(measure).any(axis=0) # voxels\n",
    "            measure = measure[:,vox_not_nan]\n",
    "            nVox_not_nan = measure.shape[1] # number of voxels  \n",
    "\n",
    "            # update channel descriptor and measurements\n",
    "            chn_des = {'voxels': np.array(['voxel_' + str(x) for x in np.arange(nVox_not_nan)])} # descriptors for channels\n",
    "            dataset_test.measurements = measure\n",
    "            dataset_test.channel_descriptors = chn_des\n",
    "\n",
    "            dataset_merge_new.append(dataset_test)  \n",
    "\n",
    "        # save dataset\n",
    "        if subject in exp2_subjects:\n",
    "            with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_merge_across_sess_WT_SP1.pkg'),'wb') as f:\n",
    "                pickle.dump(dataset_merge_new,f)\n",
    "        else:\n",
    "            with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_merge_across_sess_WT_3dva.pkg'),'wb') as f:\n",
    "                pickle.dump(dataset_merge_new,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9bce402e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rsatoolbox.data.Dataset\n",
      "measurements = \n",
      "[[ 1.13681543  1.38350403  3.93112111 ...  1.02807379  0.39928079\n",
      "   1.18702257]\n",
      " [-2.84869862 -4.16433764 -0.19244017 ...  1.57189214  0.55069697\n",
      "   0.33742237]\n",
      " [-3.5486486  -3.39419794  0.74993289 ...  2.44093609  1.17255092\n",
      "  -1.37427986]\n",
      " [-1.26268065 -6.55327177 -0.56980014 ... -0.48392674 -0.04410791\n",
      "  -1.77953053]\n",
      " [-1.64067352 -2.59643316 -1.58815801 ...  2.16497278  1.5415889\n",
      "  -1.1298852 ]]\n",
      "...\n",
      "\n",
      "descriptors: \n",
      "ecc_or_set = P1\n",
      "roi = area4-ju50\n",
      "subj = f19\n",
      "name = f19 | area4-ju50 | P1\n",
      "\n",
      "\n",
      "obs_descriptors: \n",
      "pattern_index = [0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0\n",
      " 1 0 1 0 1 0 1 0 1 0 1]\n",
      "full_conds = ['CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR'\n",
      " 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR'\n",
      " 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR'\n",
      " 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR']\n",
      "sess_index = [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1]\n",
      "run_name = ['R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R4' 'R4' 'R4' 'R4' 'R4' 'R4'\n",
      " 'R4' 'R4' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5' 'R3' 'R3' 'R3' 'R3'\n",
      " 'R3' 'R3' 'R3' 'R3' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5' 'R6' 'R6'\n",
      " 'R6' 'R6' 'R6' 'R6' 'R6' 'R6']\n",
      "conds_index = [ 0  1  0  1  0  1  0  1  2  3  2  3  2  3  2  3  4  5  4  5  4  5  4  5\n",
      "  6  7  6  7  6  7  6  7  8  9  8  9  8  9  8  9 10 11 10 11 10 11 10 11]\n",
      "run_pair_index = [0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 4 4 4 4 4\n",
      " 4 4 4 5 5 5 5 5 5 5 5]\n",
      "conds = ['W-R1-S5934' 'T-R1-S5934' 'W-R1-S5934' 'T-R1-S5934' 'W-R1-S5934'\n",
      " 'T-R1-S5934' 'W-R1-S5934' 'T-R1-S5934' 'W-R4-S5934' 'T-R4-S5934'\n",
      " 'W-R4-S5934' 'T-R4-S5934' 'W-R4-S5934' 'T-R4-S5934' 'W-R4-S5934'\n",
      " 'T-R4-S5934' 'W-R5-S5934' 'T-R5-S5934' 'W-R5-S5934' 'T-R5-S5934'\n",
      " 'W-R5-S5934' 'T-R5-S5934' 'W-R5-S5934' 'T-R5-S5934' 'W-R3-S5946'\n",
      " 'T-R3-S5946' 'W-R3-S5946' 'T-R3-S5946' 'W-R3-S5946' 'T-R3-S5946'\n",
      " 'W-R3-S5946' 'T-R3-S5946' 'W-R5-S5946' 'T-R5-S5946' 'W-R5-S5946'\n",
      " 'T-R5-S5946' 'W-R5-S5946' 'T-R5-S5946' 'W-R5-S5946' 'T-R5-S5946'\n",
      " 'W-R6-S5946' 'T-R6-S5946' 'W-R6-S5946' 'T-R6-S5946' 'W-R6-S5946'\n",
      " 'T-R6-S5946' 'W-R6-S5946' 'T-R6-S5946']\n",
      "\n",
      "\n",
      "channel_descriptors: \n",
      "voxels = ['voxel_0' 'voxel_1' 'voxel_2' ... 'voxel_1354' 'voxel_1355' 'voxel_1356']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset_merge_new[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f01f461",
   "metadata": {},
   "source": [
    "## WT model, 5.5 dva or spatial set 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "28b75899",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(3):\n",
    "    for subject in subjects:\n",
    "        \n",
    "        # load dataset (already merged across runs)\n",
    "        with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_run_ecc.pkg'),'rb') as f:\n",
    "            dataset_merge = pickle.load(f)\n",
    "        #print(dataset_merge[0])\n",
    "        \n",
    "        dataset_merge_new = []\n",
    "        uniq_roi = order #['area4', 'v1', 'v2', 'ips', 'fef', 'sfg', 'mfg', 'ifg']\n",
    "        #list(set([dataset_merge[i].descriptors['roi'] for i in range(len(dataset_merge))]))\n",
    "        uniq_sess = sorted(list(set([dataset_merge[i].descriptors['session'] for i in range(len(dataset_merge))])))\n",
    "        for curr_roi in uniq_roi: \n",
    "            # for each run and session\n",
    "            idx = 0\n",
    "            data_across_sess = []\n",
    "            \n",
    "            idx_cond = 0\n",
    "            idx_run = 0\n",
    "            for idx_sess,curr_sess in enumerate(uniq_sess):\n",
    "                data_temp = copy.deepcopy([x for x in dataset_merge if (x.descriptors['session'] == curr_sess) and\n",
    "                                                 (x.descriptors['roi'] == curr_roi)])\n",
    "                #print(len(data_temp))\n",
    "                for rr in range(len(data_temp)):\n",
    "                    curr_ecc = data_temp[rr].descriptors['ecc_or_set']\n",
    "                    \n",
    "                    #print(curr_ecc)\n",
    "                    if (curr_ecc == 'E1') | (curr_ecc == 'P1'): # 3 dva or spatial set 1\n",
    "                        continue\n",
    "                    \n",
    "                    curr_run = data_temp[rr].descriptors['run']\n",
    "                    curr_conds = data_temp[rr].obs_descriptors['conds']\n",
    "                    curr_conds_index = data_temp[rr].obs_descriptors['conds_index']\n",
    "                    curr_run_name = data_temp[rr].obs_descriptors['run_name']\n",
    "                    \n",
    "                    curr_data = data_temp[rr].get_measurements()\n",
    "                    #print(curr_run,curr_ecc,curr_data.shape)\n",
    "\n",
    "                    # remove nan conditions\n",
    "                    # which rows (conditions) do not have all nan values (valid conditions)\n",
    "                    cond_not_nan = ~np.isnan(curr_data).all(axis=1)\n",
    "                    #print(cond_not_nan)\n",
    "\n",
    "                    # let's remove those conditions\n",
    "                    curr_data = curr_data[cond_not_nan,:]\n",
    "                    #print(curr_data.shape)\n",
    "\n",
    "                    # remove those invalid conditions from the conds and conds_index too\n",
    "                    curr_conds = [curr_conds[i] for i in range(len(cond_not_nan)) if cond_not_nan[i]]\n",
    "                    curr_conds_index = [curr_conds_index[i] for i in range(len(cond_not_nan)) if cond_not_nan[i]]\n",
    "                    #print(curr_conds,curr_conds_index)\n",
    "                    new_run_name = [curr_run_name[i] for i in range(len(cond_not_nan)) if cond_not_nan[i]]\n",
    "                    \n",
    "                    # update conditions and condition index\n",
    "                    # for the LR model\n",
    "                    # calculate new condition names and indices\n",
    "                    if subject in exp2_subjects:\n",
    "                        new_conds = [x[-3]+'-'+curr_run+'-'+curr_sess for x in curr_conds]\n",
    "                    else:\n",
    "                        new_conds = [x[-2]+'-'+curr_run+'-'+curr_sess for x in curr_conds]\n",
    "                        \n",
    "                    new_conds_index = [idx if x == 'W'+'-'+curr_run+'-'+curr_sess \n",
    "                                       else idx+1 for x in new_conds]\n",
    "                    idx = idx+2 #uniq_sess.index(curr_sess)*2\n",
    "                    #print(new_conds)\n",
    "                    \n",
    "                    pattern_index = [idx_cond if x == 'W'+'-'+curr_run+'-'+curr_sess \n",
    "                                     else idx_cond+1 for x in new_conds]       \n",
    "                    run_pair_index = [idx_run for _ in range(len(curr_conds))]\n",
    "                    sess_index = [idx_sess for _ in range(len(curr_conds))]\n",
    "                    idx_run += 1\n",
    "\n",
    "                    # update\n",
    "                    data_temp[rr].obs_descriptors['full_conds'] = [c[:3] for c in curr_conds]\n",
    "                    data_temp[rr].measurements = curr_data\n",
    "                    data_temp[rr].obs_descriptors['conds'] = new_conds\n",
    "                    data_temp[rr].obs_descriptors['conds_index'] = new_conds_index\n",
    "                    data_temp[rr].obs_descriptors['run_name'] = new_run_name\n",
    "                    data_temp[rr].obs_descriptors['pattern_index'] = pattern_index\n",
    "                    data_temp[rr].obs_descriptors['run_pair_index'] = run_pair_index\n",
    "                    data_temp[rr].obs_descriptors['sess_index'] = sess_index\n",
    "                    \n",
    "                    # update the descriptors too\n",
    "                    data_temp[rr].descriptors.pop('session', None)\n",
    "                    data_temp[rr].descriptors.pop('run', None)\n",
    "                    #data_temp[rr].descriptors.pop('ecc', None)\n",
    "                    data_temp[rr].descriptors['name'] = subject + ' | ' + curr_roi + ' | ' + curr_ecc\n",
    "                    \n",
    "                    \n",
    "                    #print(data_temp[rr])\n",
    "                    data_across_sess.extend([data_temp[rr]])\n",
    "                    #print(len(data_across_sess)) # 5 or 6 run combinations \n",
    "\n",
    "            #    now we can merge datasets across sessions\n",
    "            dataset_test = rsatoolbox.data.dataset.merge_subsets(data_across_sess)\n",
    "            #             print(len(dataset_test.obs_descriptors['conds']))\n",
    "            #             print(dataset_test.obs_descriptors['conds'])\n",
    "\n",
    "            # remove voxels that are nan\n",
    "            measure = dataset_test.get_measurements()\n",
    "            vox_not_nan = ~np.isnan(measure).any(axis=0) # voxels\n",
    "            measure = measure[:,vox_not_nan]\n",
    "            nVox_not_nan = measure.shape[1] # number of voxels  \n",
    "\n",
    "            # update channel descriptor and measurements\n",
    "            chn_des = {'voxels': np.array(['voxel_' + str(x) for x in np.arange(nVox_not_nan)])} # descriptors for channels\n",
    "            dataset_test.measurements = measure\n",
    "            dataset_test.channel_descriptors = chn_des\n",
    "\n",
    "            dataset_merge_new.append(dataset_test)  \n",
    "\n",
    "        # save dataset\n",
    "        if subject in exp2_subjects:\n",
    "            with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_merge_across_sess_WT_SP2.pkg'),'wb') as f:\n",
    "                pickle.dump(dataset_merge_new,f)\n",
    "        else:\n",
    "            with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_merge_across_sess_WT_5.5dva.pkg'),'wb') as f:\n",
    "                pickle.dump(dataset_merge_new,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a28e3ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rsatoolbox.data.Dataset\n",
      "measurements = \n",
      "[[-4.32295465 -6.16556215  0.7349382  ... -1.17331648  0.45550582\n",
      "  -2.78499293]\n",
      " [-1.72085452 -3.63415432 -1.65540874 ...  0.33521384 -0.54659218\n",
      "  -0.26850107]\n",
      " [-3.02165127 -6.29844046 -1.63729489 ...  0.08400786  1.30360365\n",
      "  -1.44681382]\n",
      " [-3.38832998 -6.20992613  0.13150956 ...  0.90429479 -2.20506287\n",
      "  -4.67829561]\n",
      " [-3.95717478 -6.65573072 -0.20510076 ... -3.87506557 -1.18707049\n",
      "  -2.55028176]]\n",
      "...\n",
      "\n",
      "descriptors: \n",
      "ecc_or_set = P2\n",
      "roi = area4-ju50\n",
      "subj = f19\n",
      "name = f19 | area4-ju50 | P2\n",
      "\n",
      "\n",
      "obs_descriptors: \n",
      "pattern_index = [0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0\n",
      " 1 0 1 0 1 0 1 0 1 0 1]\n",
      "full_conds = ['CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR'\n",
      " 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR'\n",
      " 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR'\n",
      " 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR']\n",
      "sess_index = [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1]\n",
      "run_name = ['R2' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2' 'R3' 'R3' 'R3' 'R3' 'R3' 'R3'\n",
      " 'R3' 'R3' 'R6' 'R6' 'R6' 'R6' 'R6' 'R6' 'R6' 'R6' 'R1' 'R1' 'R1' 'R1'\n",
      " 'R1' 'R1' 'R1' 'R1' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2' 'R4' 'R4'\n",
      " 'R4' 'R4' 'R4' 'R4' 'R4' 'R4']\n",
      "conds_index = [ 0  1  0  1  0  1  0  1  2  3  2  3  2  3  2  3  4  5  4  5  4  5  4  5\n",
      "  6  7  6  7  6  7  6  7  8  9  8  9  8  9  8  9 10 11 10 11 10 11 10 11]\n",
      "run_pair_index = [0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 4 4 4 4 4\n",
      " 4 4 4 5 5 5 5 5 5 5 5]\n",
      "conds = ['W-R2-S5934' 'T-R2-S5934' 'W-R2-S5934' 'T-R2-S5934' 'W-R2-S5934'\n",
      " 'T-R2-S5934' 'W-R2-S5934' 'T-R2-S5934' 'W-R3-S5934' 'T-R3-S5934'\n",
      " 'W-R3-S5934' 'T-R3-S5934' 'W-R3-S5934' 'T-R3-S5934' 'W-R3-S5934'\n",
      " 'T-R3-S5934' 'W-R6-S5934' 'T-R6-S5934' 'W-R6-S5934' 'T-R6-S5934'\n",
      " 'W-R6-S5934' 'T-R6-S5934' 'W-R6-S5934' 'T-R6-S5934' 'W-R1-S5946'\n",
      " 'T-R1-S5946' 'W-R1-S5946' 'T-R1-S5946' 'W-R1-S5946' 'T-R1-S5946'\n",
      " 'W-R1-S5946' 'T-R1-S5946' 'W-R2-S5946' 'T-R2-S5946' 'W-R2-S5946'\n",
      " 'T-R2-S5946' 'W-R2-S5946' 'T-R2-S5946' 'W-R2-S5946' 'T-R2-S5946'\n",
      " 'W-R4-S5946' 'T-R4-S5946' 'W-R4-S5946' 'T-R4-S5946' 'W-R4-S5946'\n",
      " 'T-R4-S5946' 'W-R4-S5946' 'T-R4-S5946']\n",
      "\n",
      "\n",
      "channel_descriptors: \n",
      "voxels = ['voxel_0' 'voxel_1' 'voxel_2' ... 'voxel_1354' 'voxel_1355' 'voxel_1356']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset_merge_new[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45454bd1",
   "metadata": {},
   "source": [
    "## Eccentricity model (exp 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f8c2f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = ['f09','f10','f11','f12','f15','f16','f17','f18','f19']\n",
    "epochs = ['delay','response','stimulus']\n",
    "exp1_subjects = ['f09','f10','f11','f12','f15','f16']\n",
    "exp2_subjects = ['f17','f18','f19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "783cf598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR THE Eccentricity model\n",
    "for epoch in range(3):\n",
    "    for subject in exp1_subjects:\n",
    "        \n",
    "        # load dataset (already merged across runs)\n",
    "        with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_run_ecc.pkg'),'rb') as f:\n",
    "            dataset_merge = pickle.load(f)\n",
    "        \n",
    "        dataset_merge_new = []\n",
    "        uniq_roi = order #['area4', 'v1', 'v2', 'ips', 'fef', 'sfg', 'mfg', 'ifg']\n",
    "        #list(set([dataset_merge[i].descriptors['roi'] for i in range(len(dataset_merge))]))\n",
    "        uniq_sess = sorted(list(set([dataset_merge[i].descriptors['session'] for i in range(len(dataset_merge))])))\n",
    "        for curr_roi in uniq_roi: \n",
    "            # for each run and session\n",
    "            count_for_5dva = np.arange(1,20,2)\n",
    "            count_for_3dva = np.arange(0,20,2)\n",
    "            idx5 = 0\n",
    "            idx3 = 0\n",
    "            data_across_sess = []\n",
    "            \n",
    "            idx_cond = 0\n",
    "            idx_run = 0\n",
    "            for idx_sess,curr_sess in enumerate(uniq_sess):\n",
    "                data_temp = copy.deepcopy([x for x in dataset_merge if (x.descriptors['session'] == curr_sess) and\n",
    "                                                 (x.descriptors['roi'] == curr_roi)])\n",
    "                for rr in range(len(data_temp)):\n",
    "                    curr_run = data_temp[rr].descriptors['run']\n",
    "                    curr_conds = data_temp[rr].obs_descriptors['conds']\n",
    "                    curr_conds_index = data_temp[rr].obs_descriptors['conds_index']\n",
    "                    curr_ecc = data_temp[rr].descriptors['ecc_or_set']\n",
    "                    curr_data = data_temp[rr].get_measurements()\n",
    "                    curr_run_name = data_temp[rr].obs_descriptors['run_name']\n",
    "                    #print(curr_run,curr_ecc,curr_data.shape)\n",
    "\n",
    "                    # remove nan conditions\n",
    "                    # which rows (conditions) do not have all nan values (valid conditions)\n",
    "                    cond_not_nan = ~np.isnan(curr_data).all(axis=1)\n",
    "                    #print(cond_not_nan)\n",
    "\n",
    "                    # let's remove those conditions\n",
    "                    curr_data = curr_data[cond_not_nan,:]\n",
    "                    #print(curr_data.shape)\n",
    "\n",
    "                    # remove those invalid conditions from the conds and conds_index too\n",
    "                    curr_conds = [curr_conds[i] for i in range(len(cond_not_nan)) if cond_not_nan[i]]\n",
    "                    curr_conds_index = [curr_conds_index[i] for i in range(len(cond_not_nan)) if cond_not_nan[i]]\n",
    "                    new_run_name = [curr_run_name[i] for i in range(len(cond_not_nan)) if cond_not_nan[i]]\n",
    "                    #print(curr_conds,curr_conds_index)\n",
    "\n",
    "                    # update conditions and condition index\n",
    "                    # for the ecc model\n",
    "                    # calculate new condition names and indices\n",
    "                    new_conds = [x[0]+'-'+curr_run+'-'+curr_sess for x in curr_conds]\n",
    "                    new_conds_index = [count_for_3dva[idx3] if x == '3'+'-'+curr_run+'-'+curr_sess \n",
    "                                       else count_for_5dva[idx5] for x in new_conds]\n",
    "                    if '3' in curr_conds[0]:\n",
    "                        idx3 += 1\n",
    "                    elif '5' in curr_conds[0]:\n",
    "                        idx5 += 1\n",
    "                    #print(new_conds,new_conds_index)\n",
    "                    \n",
    "                    pattern_index = [idx_cond if x == '3'+'-'+curr_run+'-'+curr_sess \n",
    "                                     else idx_cond+1 for x in new_conds]       \n",
    "                    run_pair_index = [idx_run for _ in range(len(curr_conds))]\n",
    "                    sess_index = [idx_sess for _ in range(len(curr_conds))]\n",
    "                    idx_run += 1\n",
    "\n",
    "                    # update\n",
    "                    data_temp[rr].measurements = curr_data\n",
    "                    data_temp[rr].obs_descriptors['conds'] = new_conds\n",
    "                    data_temp[rr].obs_descriptors['conds_index'] = new_conds_index\n",
    "                    data_temp[rr].obs_descriptors['run_name'] = new_run_name\n",
    "                    data_temp[rr].obs_descriptors['pattern_index'] = pattern_index\n",
    "                    data_temp[rr].obs_descriptors['run_pair_index'] = run_pair_index\n",
    "                    data_temp[rr].obs_descriptors['sess_index'] = sess_index\n",
    "\n",
    "                    # update the descriptors too\n",
    "                    data_temp[rr].descriptors.pop('session', None)\n",
    "                    data_temp[rr].descriptors.pop('run', None)\n",
    "                    data_temp[rr].descriptors.pop('ecc_or_set', None)\n",
    "                    data_temp[rr].descriptors['name'] = subject + ' | ' + curr_roi\n",
    "\n",
    "                data_across_sess.extend(data_temp)\n",
    "                #print(len(data_across_sess)) # 5 or 6 run combinations \n",
    "\n",
    "            #    now we can merge datasets across sessions\n",
    "            dataset_test = rsatoolbox.data.dataset.merge_subsets(data_across_sess)\n",
    "            #             print(len(dataset_test.obs_descriptors['conds']))\n",
    "            #             print(dataset_test.obs_descriptors['conds'])\n",
    "\n",
    "            # remove voxels that are nan\n",
    "            measure = dataset_test.get_measurements()\n",
    "            vox_not_nan = ~np.isnan(measure).any(axis=0) # voxels\n",
    "            measure = measure[:,vox_not_nan]\n",
    "            nVox_not_nan = measure.shape[1] # number of voxels  \n",
    "\n",
    "            # update channel descriptor and measurements\n",
    "            chn_des = {'voxels': np.array(['voxel_' + str(x) for x in np.arange(nVox_not_nan)])} # descriptors for channels\n",
    "            dataset_test.measurements = measure\n",
    "            dataset_test.channel_descriptors = chn_des\n",
    "\n",
    "            dataset_merge_new.append(dataset_test)  \n",
    "\n",
    "        # save dataset\n",
    "        with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_merge_across_sess_ecc.pkg'),'wb') as f:\n",
    "            pickle.dump(dataset_merge_new,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dbb3c264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rsatoolbox.data.Dataset\n",
      "measurements = \n",
      "[[ 0.78532481  1.10438108 -1.49982238 ...  0.17502366  1.18427742\n",
      "   3.84072828]\n",
      " [ 1.46905017  2.04965448 -0.76967573 ...  0.44648355  0.64516133\n",
      "   4.07084036]\n",
      " [-0.18436474 -1.36462533 -1.70203865 ... -1.61426926 -0.79944575\n",
      "  -0.81298256]\n",
      " [ 3.70959854  3.71300507  0.35336173 ...  0.68085593  1.13180661\n",
      "   4.42984915]\n",
      " [ 0.16760957  1.1413877   0.52713263 ...  1.54696178  1.63102031\n",
      "   4.18733215]]\n",
      "...\n",
      "\n",
      "descriptors: \n",
      "roi = area4-ju50\n",
      "subj = f16\n",
      "name = f16 | area4-ju50\n",
      "\n",
      "\n",
      "obs_descriptors: \n",
      "pattern_index = [0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1\n",
      " 1 1 1 0 0 0 0 0 0 0 0]\n",
      "sess_index = [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1]\n",
      "run_name = ['R1' 'R1' 'R1' 'R1' 'R2' 'R2' 'R2' 'R2' 'R3' 'R3' 'R3' 'R3' 'R4' 'R4'\n",
      " 'R4' 'R4' 'R5' 'R5' 'R5' 'R5' 'R6' 'R6' 'R6' 'R6' 'R1' 'R1' 'R1' 'R1'\n",
      " 'R2' 'R2' 'R2' 'R2' 'R3' 'R3' 'R3' 'R3' 'R4' 'R4' 'R4' 'R4' 'R5' 'R5'\n",
      " 'R5' 'R5' 'R6' 'R6' 'R6' 'R6']\n",
      "conds_index = [ 0  0  0  0  1  1  1  1  2  2  2  2  3  3  3  3  5  5  5  5  4  4  4  4\n",
      "  7  7  7  7  6  6  6  6  9  9  9  9 11 11 11 11  8  8  8  8 10 10 10 10]\n",
      "run_pair_index = [ 0  0  0  0  1  1  1  1  2  2  2  2  3  3  3  3  4  4  4  4  5  5  5  5\n",
      "  6  6  6  6  7  7  7  7  8  8  8  8  9  9  9  9 10 10 10 10 11 11 11 11]\n",
      "conds = ['3-R1-S5824' '3-R1-S5824' '3-R1-S5824' '3-R1-S5824' '5-R2-S5824'\n",
      " '5-R2-S5824' '5-R2-S5824' '5-R2-S5824' '3-R3-S5824' '3-R3-S5824'\n",
      " '3-R3-S5824' '3-R3-S5824' '5-R4-S5824' '5-R4-S5824' '5-R4-S5824'\n",
      " '5-R4-S5824' '5-R5-S5824' '5-R5-S5824' '5-R5-S5824' '5-R5-S5824'\n",
      " '3-R6-S5824' '3-R6-S5824' '3-R6-S5824' '3-R6-S5824' '5-R1-S5847'\n",
      " '5-R1-S5847' '5-R1-S5847' '5-R1-S5847' '3-R2-S5847' '3-R2-S5847'\n",
      " '3-R2-S5847' '3-R2-S5847' '5-R3-S5847' '5-R3-S5847' '5-R3-S5847'\n",
      " '5-R3-S5847' '5-R4-S5847' '5-R4-S5847' '5-R4-S5847' '5-R4-S5847'\n",
      " '3-R5-S5847' '3-R5-S5847' '3-R5-S5847' '3-R5-S5847' '3-R6-S5847'\n",
      " '3-R6-S5847' '3-R6-S5847' '3-R6-S5847']\n",
      "\n",
      "\n",
      "channel_descriptors: \n",
      "voxels = ['voxel_0' 'voxel_1' 'voxel_2' ... 'voxel_1333' 'voxel_1334' 'voxel_1335']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset_merge_new[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f349a8f9",
   "metadata": {},
   "source": [
    "## Spatiotemporal regularity model (exp 2) - Crossing vs non-crossing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "397db083",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(3):\n",
    "    for subject in exp2_subjects:\n",
    "        \n",
    "        # load dataset (already merged across runs)\n",
    "        with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_merge_across_run.pkg'),'rb') as f:\n",
    "            dataset_merge = pickle.load(f)\n",
    "        \n",
    "        dataset_merge_new = []\n",
    "        \n",
    "        # let's merge further across runs and sessions\n",
    "        uniq_roi = order #['area4', 'v1', 'v2', 'ips', 'fef', 'sfg', 'mfg', 'ifg']\n",
    "        #list(set([dataset_merge[i].descriptors['roi'] for i in range(len(dataset_merge))]))\n",
    "        uniq_sess = sorted(list(set([dataset_merge[i].descriptors['session'] for i in range(len(dataset_merge))])))\n",
    "        for curr_roi in uniq_roi:\n",
    "            data_across_sess = []\n",
    "            idx = 0\n",
    "            idx_cond = 0\n",
    "            idx_run = 0\n",
    "            for idx_sess,curr_sess in enumerate(uniq_sess):\n",
    "                #print(curr_sess)\n",
    "                data_temp = copy.deepcopy([x for x in dataset_merge if (x.descriptors['session'] == curr_sess) and\n",
    "                                                         (x.descriptors['roi'] == curr_roi)])\n",
    "\n",
    "                # for each run combination\n",
    "                for rr in range(len(data_temp)):\n",
    "                    curr_run = data_temp[rr].descriptors['run']\n",
    "                    curr_conds = data_temp[rr].obs_descriptors['conds']\n",
    "                    curr_conds_index = data_temp[rr].obs_descriptors['conds_index']\n",
    "                    #curr_run_name = data_temp[rr].obs_descriptors['run_name']\n",
    "                    \n",
    "                    # update conditions and condition index\n",
    "                    # for the LR model\n",
    "                    # calculate new condition names and indices\n",
    "                    new_conds = [x[-3]+'-'+curr_run+'-'+curr_sess for x in curr_conds]\n",
    "                    new_conds_index = [idx if x == 'C'+'-'+curr_run+'-'+curr_sess \n",
    "                                       else idx+1 for x in new_conds]\n",
    "                    \n",
    "                    pattern_index = [idx_cond if x == 'C'+'-'+curr_run+'-'+curr_sess \n",
    "                                     else idx_cond+1 for x in new_conds]       \n",
    "                    run_index = [idx_run for _ in range(len(curr_conds))]\n",
    "                    idx_run += 1\n",
    "                    sess_index = [idx_sess for _ in range(len(curr_conds))]\n",
    "                    \n",
    "#                     ecc_val = data_temp[rr].descriptors.pop('ecc', None)\n",
    "#                     ecc_index = [ecc_val for _ in range(len(curr_conds))]\n",
    "            \n",
    "                    idx = idx+2 #uniq_sess.index(curr_sess)*2\n",
    "                    #print(new_conds)\n",
    "                    \n",
    "                    # update\n",
    "                    data_temp[rr].obs_descriptors['full_conds'] = curr_conds\n",
    "                    data_temp[rr].obs_descriptors['conds'] = new_conds\n",
    "                    data_temp[rr].obs_descriptors['conds_index'] = new_conds_index\n",
    "                    \n",
    "                    data_temp[rr].obs_descriptors['pattern_index'] = pattern_index\n",
    "                    data_temp[rr].obs_descriptors['run_pair_index'] = run_index\n",
    "                    data_temp[rr].obs_descriptors['sess_index'] = sess_index\n",
    "#                    data_temp[rr].obs_descriptors['ecc_index'] = sess_index\n",
    "                    \n",
    "                    # update the descriptors too\n",
    "                    data_temp[rr].descriptors.pop('session', None)\n",
    "                    data_temp[rr].descriptors.pop('run', None)\n",
    "                    data_temp[rr].descriptors.pop('ecc', None)\n",
    "                    data_temp[rr].descriptors['name'] = subject + ' | ' + curr_roi\n",
    "                \n",
    "                data_across_sess.extend(data_temp)\n",
    "            #print(len(data_across_sess)) # 5 or 6 run combinations \n",
    "\n",
    "        #    now we can merge datasets across sessions\n",
    "            dataset_test = rsatoolbox.data.dataset.merge_subsets(data_across_sess)\n",
    "#             print(len(dataset_test.obs_descriptors['conds']))\n",
    "#             print(dataset_test.obs_descriptors['conds'])\n",
    "\n",
    "            # remove voxels that are nan\n",
    "            measure = dataset_test.get_measurements()\n",
    "            vox_not_nan = ~np.isnan(measure).any(axis=0) # voxels\n",
    "            measure = measure[:,vox_not_nan]\n",
    "            nVox_not_nan = measure.shape[1] # number of voxels  \n",
    "            \n",
    "            # update channel descriptor and measurements\n",
    "            chn_des = {'voxels': np.array(['voxel_' + str(x) for x in np.arange(nVox_not_nan)])} # descriptors for channels\n",
    "            dataset_test.measurements = measure\n",
    "            dataset_test.channel_descriptors = chn_des\n",
    "            \n",
    "            dataset_merge_new.append(dataset_test)  \n",
    "            \n",
    "        # save dataset\n",
    "        with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_merge_across_sess_CN.pkg'),'wb') as f:\n",
    "            pickle.dump(dataset_merge_new,f)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "05c5163e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rsatoolbox.data.Dataset\n",
      "measurements = \n",
      "[[ 1.13681543  1.38350403  3.93112111 ...  1.02807379  0.39928079\n",
      "   1.18702257]\n",
      " [-2.84869862 -4.16433764 -0.19244017 ...  1.57189214  0.55069697\n",
      "   0.33742237]\n",
      " [-3.5486486  -3.39419794  0.74993289 ...  2.44093609  1.17255092\n",
      "  -1.37427986]\n",
      " [-1.26268065 -6.55327177 -0.56980014 ... -0.48392674 -0.04410791\n",
      "  -1.77953053]\n",
      " [-1.64067352 -2.59643316 -1.58815801 ...  2.16497278  1.5415889\n",
      "  -1.1298852 ]]\n",
      "...\n",
      "\n",
      "descriptors: \n",
      "ecc_or_set = P1P2\n",
      "roi = area4-ju50\n",
      "subj = f19\n",
      "name = f19 | area4-ju50\n",
      "\n",
      "\n",
      "obs_descriptors: \n",
      "pattern_index = [0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1\n",
      " 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0\n",
      " 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1]\n",
      "full_conds = ['CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR'\n",
      " 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR'\n",
      " 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR'\n",
      " 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR'\n",
      " 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR'\n",
      " 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR'\n",
      " 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR'\n",
      " 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR']\n",
      "sess_index = [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "run_name = ['R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2'\n",
      " 'R2' 'R2' 'R4' 'R4' 'R4' 'R4' 'R4' 'R4' 'R4' 'R4' 'R3' 'R3' 'R3' 'R3'\n",
      " 'R3' 'R3' 'R3' 'R3' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5' 'R6' 'R6'\n",
      " 'R6' 'R6' 'R6' 'R6' 'R6' 'R6' 'R3' 'R3' 'R3' 'R3' 'R3' 'R3' 'R3' 'R3'\n",
      " 'R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5'\n",
      " 'R5' 'R5' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2' 'R6' 'R6' 'R6' 'R6'\n",
      " 'R6' 'R6' 'R6' 'R6' 'R4' 'R4' 'R4' 'R4' 'R4' 'R4' 'R4' 'R4']\n",
      "conds_index = [ 0  0  0  0  1  1  1  1  0  0  0  0  1  1  1  1  2  2  2  2  3  3  3  3\n",
      "  2  2  2  2  3  3  3  3  4  4  4  4  5  5  5  5  4  4  4  4  5  5  5  5\n",
      "  6  6  6  6  7  7  7  7  6  6  6  6  7  7  7  7  8  8  8  8  9  9  9  9\n",
      "  8  8  8  8  9  9  9  9 10 10 10 10 11 11 11 11 10 10 10 10 11 11 11 11]\n",
      "run_pair_index = [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "conds = ['C-R1R2-S5934' 'C-R1R2-S5934' 'C-R1R2-S5934' 'C-R1R2-S5934'\n",
      " 'N-R1R2-S5934' 'N-R1R2-S5934' 'N-R1R2-S5934' 'N-R1R2-S5934'\n",
      " 'C-R1R2-S5934' 'C-R1R2-S5934' 'C-R1R2-S5934' 'C-R1R2-S5934'\n",
      " 'N-R1R2-S5934' 'N-R1R2-S5934' 'N-R1R2-S5934' 'N-R1R2-S5934'\n",
      " 'C-R4R3-S5934' 'C-R4R3-S5934' 'C-R4R3-S5934' 'C-R4R3-S5934'\n",
      " 'N-R4R3-S5934' 'N-R4R3-S5934' 'N-R4R3-S5934' 'N-R4R3-S5934'\n",
      " 'C-R4R3-S5934' 'C-R4R3-S5934' 'C-R4R3-S5934' 'C-R4R3-S5934'\n",
      " 'N-R4R3-S5934' 'N-R4R3-S5934' 'N-R4R3-S5934' 'N-R4R3-S5934'\n",
      " 'C-R5R6-S5934' 'C-R5R6-S5934' 'C-R5R6-S5934' 'C-R5R6-S5934'\n",
      " 'N-R5R6-S5934' 'N-R5R6-S5934' 'N-R5R6-S5934' 'N-R5R6-S5934'\n",
      " 'C-R5R6-S5934' 'C-R5R6-S5934' 'C-R5R6-S5934' 'C-R5R6-S5934'\n",
      " 'N-R5R6-S5934' 'N-R5R6-S5934' 'N-R5R6-S5934' 'N-R5R6-S5934'\n",
      " 'C-R3R1-S5946' 'C-R3R1-S5946' 'C-R3R1-S5946' 'C-R3R1-S5946'\n",
      " 'N-R3R1-S5946' 'N-R3R1-S5946' 'N-R3R1-S5946' 'N-R3R1-S5946'\n",
      " 'C-R3R1-S5946' 'C-R3R1-S5946' 'C-R3R1-S5946' 'C-R3R1-S5946'\n",
      " 'N-R3R1-S5946' 'N-R3R1-S5946' 'N-R3R1-S5946' 'N-R3R1-S5946'\n",
      " 'C-R5R2-S5946' 'C-R5R2-S5946' 'C-R5R2-S5946' 'C-R5R2-S5946'\n",
      " 'N-R5R2-S5946' 'N-R5R2-S5946' 'N-R5R2-S5946' 'N-R5R2-S5946'\n",
      " 'C-R5R2-S5946' 'C-R5R2-S5946' 'C-R5R2-S5946' 'C-R5R2-S5946'\n",
      " 'N-R5R2-S5946' 'N-R5R2-S5946' 'N-R5R2-S5946' 'N-R5R2-S5946'\n",
      " 'C-R6R4-S5946' 'C-R6R4-S5946' 'C-R6R4-S5946' 'C-R6R4-S5946'\n",
      " 'N-R6R4-S5946' 'N-R6R4-S5946' 'N-R6R4-S5946' 'N-R6R4-S5946'\n",
      " 'C-R6R4-S5946' 'C-R6R4-S5946' 'C-R6R4-S5946' 'C-R6R4-S5946'\n",
      " 'N-R6R4-S5946' 'N-R6R4-S5946' 'N-R6R4-S5946' 'N-R6R4-S5946']\n",
      "\n",
      "\n",
      "channel_descriptors: \n",
      "voxels = ['voxel_0' 'voxel_1' 'voxel_2' ... 'voxel_1354' 'voxel_1355' 'voxel_1356']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset_merge_new[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42429dae",
   "metadata": {},
   "source": [
    "## Crossing/non-crossing (exp2) - spatial set 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "afc82717",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(3):\n",
    "    for subject in exp2_subjects:\n",
    "        \n",
    "        # load dataset (already merged across runs)\n",
    "        with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_run_ecc.pkg'),'rb') as f:\n",
    "            dataset_merge = pickle.load(f)\n",
    "        #print(dataset_merge[0])\n",
    "        \n",
    "        dataset_merge_new = []\n",
    "        uniq_roi = order #['area4', 'v1', 'v2', 'ips', 'fef', 'sfg', 'mfg', 'ifg']\n",
    "        #list(set([dataset_merge[i].descriptors['roi'] for i in range(len(dataset_merge))]))\n",
    "        uniq_sess = sorted(list(set([dataset_merge[i].descriptors['session'] for i in range(len(dataset_merge))])))\n",
    "        for curr_roi in uniq_roi: \n",
    "            # for each run and session\n",
    "            idx = 0\n",
    "            data_across_sess = []\n",
    "            \n",
    "            idx_cond = 0\n",
    "            idx_run = 0\n",
    "            for idx_sess,curr_sess in enumerate(uniq_sess):\n",
    "                data_temp = copy.deepcopy([x for x in dataset_merge if (x.descriptors['session'] == curr_sess) and\n",
    "                                                 (x.descriptors['roi'] == curr_roi)])\n",
    "                #print(len(data_temp))\n",
    "                for rr in range(len(data_temp)):\n",
    "                    curr_ecc = data_temp[rr].descriptors['ecc_or_set']\n",
    "                    \n",
    "                    #print(curr_ecc)\n",
    "                    if (curr_ecc == 'E2') | (curr_ecc == 'P2'): # 5.5 dva or spatial set 2\n",
    "                        continue\n",
    "                    \n",
    "                    curr_run = data_temp[rr].descriptors['run']\n",
    "                    curr_conds = data_temp[rr].obs_descriptors['conds']\n",
    "                    curr_conds_index = data_temp[rr].obs_descriptors['conds_index']\n",
    "                    curr_run_name = data_temp[rr].obs_descriptors['run_name']\n",
    "                    \n",
    "                    curr_data = data_temp[rr].get_measurements()\n",
    "                    #print(curr_run,curr_ecc,curr_data.shape)\n",
    "\n",
    "                    # remove nan conditions\n",
    "                    # which rows (conditions) do not have all nan values (valid conditions)\n",
    "                    cond_not_nan = ~np.isnan(curr_data).all(axis=1)\n",
    "                    #print(cond_not_nan)\n",
    "\n",
    "                    # let's remove those conditions\n",
    "                    curr_data = curr_data[cond_not_nan,:]\n",
    "                    #print(curr_data.shape)\n",
    "\n",
    "                    # remove those invalid conditions from the conds and conds_index too\n",
    "                    curr_conds = [curr_conds[i] for i in range(len(cond_not_nan)) if cond_not_nan[i]]\n",
    "                    curr_conds_index = [curr_conds_index[i] for i in range(len(cond_not_nan)) if cond_not_nan[i]]\n",
    "                    #print(curr_conds,curr_conds_index)\n",
    "                    new_run_name = [curr_run_name[i] for i in range(len(cond_not_nan)) if cond_not_nan[i]]\n",
    "                    \n",
    "                    # update conditions and condition index\n",
    "                    # for the LR model\n",
    "                    # calculate new condition names and indices\n",
    "                    new_conds = [x[0]+'-'+curr_run+'-'+curr_sess for x in curr_conds]\n",
    "                        \n",
    "                    new_conds_index = [idx if x == 'C'+'-'+curr_run+'-'+curr_sess \n",
    "                                       else idx+1 for x in new_conds]\n",
    "                    idx = idx+2 #uniq_sess.index(curr_sess)*2\n",
    "                    #print(new_conds)\n",
    "                    \n",
    "                    pattern_index = [idx_cond if x == 'C'+'-'+curr_run+'-'+curr_sess \n",
    "                                     else idx_cond+1 for x in new_conds]       \n",
    "                    run_pair_index = [idx_run for _ in range(len(curr_conds))]\n",
    "                    sess_index = [idx_sess for _ in range(len(curr_conds))]\n",
    "                    idx_run += 1\n",
    "\n",
    "                    # update\n",
    "                    data_temp[rr].obs_descriptors['full_conds'] = [c[:3] for c in curr_conds]\n",
    "                    data_temp[rr].measurements = curr_data\n",
    "                    data_temp[rr].obs_descriptors['conds'] = new_conds\n",
    "                    data_temp[rr].obs_descriptors['conds_index'] = new_conds_index\n",
    "                    data_temp[rr].obs_descriptors['run_name'] = new_run_name\n",
    "                    data_temp[rr].obs_descriptors['pattern_index'] = pattern_index\n",
    "                    data_temp[rr].obs_descriptors['run_pair_index'] = run_pair_index\n",
    "                    data_temp[rr].obs_descriptors['sess_index'] = sess_index\n",
    "                    \n",
    "                    # update the descriptors too\n",
    "                    data_temp[rr].descriptors.pop('session', None)\n",
    "                    data_temp[rr].descriptors.pop('run', None)\n",
    "                    #data_temp[rr].descriptors.pop('ecc', None)\n",
    "                    data_temp[rr].descriptors['name'] = subject + ' | ' + curr_roi + ' | ' + curr_ecc\n",
    "                    \n",
    "                    \n",
    "                    #print(data_temp[rr])\n",
    "                    data_across_sess.extend([data_temp[rr]])\n",
    "                    #print(len(data_across_sess)) # 5 or 6 run combinations \n",
    "\n",
    "            #    now we can merge datasets across sessions\n",
    "            dataset_test = rsatoolbox.data.dataset.merge_subsets(data_across_sess)\n",
    "            #             print(len(dataset_test.obs_descriptors['conds']))\n",
    "            #             print(dataset_test.obs_descriptors['conds'])\n",
    "\n",
    "            # remove voxels that are nan\n",
    "            measure = dataset_test.get_measurements()\n",
    "            vox_not_nan = ~np.isnan(measure).any(axis=0) # voxels\n",
    "            measure = measure[:,vox_not_nan]\n",
    "            nVox_not_nan = measure.shape[1] # number of voxels  \n",
    "\n",
    "            # update channel descriptor and measurements\n",
    "            chn_des = {'voxels': np.array(['voxel_' + str(x) for x in np.arange(nVox_not_nan)])} # descriptors for channels\n",
    "            dataset_test.measurements = measure\n",
    "            dataset_test.channel_descriptors = chn_des\n",
    "\n",
    "            dataset_merge_new.append(dataset_test)  \n",
    "\n",
    "        # save dataset\n",
    "        with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_merge_across_sess_CN_SP1.pkg'),'wb') as f:\n",
    "            pickle.dump(dataset_merge_new,f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bae5537b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rsatoolbox.data.Dataset\n",
      "measurements = \n",
      "[[ 1.13681543  1.38350403  3.93112111 ...  1.02807379  0.39928079\n",
      "   1.18702257]\n",
      " [-2.84869862 -4.16433764 -0.19244017 ...  1.57189214  0.55069697\n",
      "   0.33742237]\n",
      " [-3.5486486  -3.39419794  0.74993289 ...  2.44093609  1.17255092\n",
      "  -1.37427986]\n",
      " [-1.26268065 -6.55327177 -0.56980014 ... -0.48392674 -0.04410791\n",
      "  -1.77953053]\n",
      " [-1.64067352 -2.59643316 -1.58815801 ...  2.16497278  1.5415889\n",
      "  -1.1298852 ]]\n",
      "...\n",
      "\n",
      "descriptors: \n",
      "ecc_or_set = P1\n",
      "roi = area4-ju50\n",
      "subj = f19\n",
      "name = f19 | area4-ju50 | P1\n",
      "\n",
      "\n",
      "obs_descriptors: \n",
      "pattern_index = [0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1\n",
      " 1 1 1 0 0 0 0 1 1 1 1]\n",
      "full_conds = ['CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR'\n",
      " 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR'\n",
      " 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR'\n",
      " 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR']\n",
      "sess_index = [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1]\n",
      "run_name = ['R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R1' 'R4' 'R4' 'R4' 'R4' 'R4' 'R4'\n",
      " 'R4' 'R4' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5' 'R3' 'R3' 'R3' 'R3'\n",
      " 'R3' 'R3' 'R3' 'R3' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5' 'R5' 'R6' 'R6'\n",
      " 'R6' 'R6' 'R6' 'R6' 'R6' 'R6']\n",
      "conds_index = [ 0  0  0  0  1  1  1  1  2  2  2  2  3  3  3  3  4  4  4  4  5  5  5  5\n",
      "  6  6  6  6  7  7  7  7  8  8  8  8  9  9  9  9 10 10 10 10 11 11 11 11]\n",
      "run_pair_index = [0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 4 4 4 4 4\n",
      " 4 4 4 5 5 5 5 5 5 5 5]\n",
      "conds = ['C-R1-S5934' 'C-R1-S5934' 'C-R1-S5934' 'C-R1-S5934' 'N-R1-S5934'\n",
      " 'N-R1-S5934' 'N-R1-S5934' 'N-R1-S5934' 'C-R4-S5934' 'C-R4-S5934'\n",
      " 'C-R4-S5934' 'C-R4-S5934' 'N-R4-S5934' 'N-R4-S5934' 'N-R4-S5934'\n",
      " 'N-R4-S5934' 'C-R5-S5934' 'C-R5-S5934' 'C-R5-S5934' 'C-R5-S5934'\n",
      " 'N-R5-S5934' 'N-R5-S5934' 'N-R5-S5934' 'N-R5-S5934' 'C-R3-S5946'\n",
      " 'C-R3-S5946' 'C-R3-S5946' 'C-R3-S5946' 'N-R3-S5946' 'N-R3-S5946'\n",
      " 'N-R3-S5946' 'N-R3-S5946' 'C-R5-S5946' 'C-R5-S5946' 'C-R5-S5946'\n",
      " 'C-R5-S5946' 'N-R5-S5946' 'N-R5-S5946' 'N-R5-S5946' 'N-R5-S5946'\n",
      " 'C-R6-S5946' 'C-R6-S5946' 'C-R6-S5946' 'C-R6-S5946' 'N-R6-S5946'\n",
      " 'N-R6-S5946' 'N-R6-S5946' 'N-R6-S5946']\n",
      "\n",
      "\n",
      "channel_descriptors: \n",
      "voxels = ['voxel_0' 'voxel_1' 'voxel_2' ... 'voxel_1354' 'voxel_1355' 'voxel_1356']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset_merge_new[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4742f07f",
   "metadata": {},
   "source": [
    "## Crossing/non-crossing (exp2) - spatial set 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0d1ed159",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(3):\n",
    "    for subject in exp2_subjects:\n",
    "        \n",
    "        # load dataset (already merged across runs)\n",
    "        with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_run_ecc.pkg'),'rb') as f:\n",
    "            dataset_merge = pickle.load(f)\n",
    "        #print(dataset_merge[0])\n",
    "        \n",
    "        dataset_merge_new = []\n",
    "        uniq_roi = order #['area4', 'v1', 'v2', 'ips', 'fef', 'sfg', 'mfg', 'ifg']\n",
    "        #list(set([dataset_merge[i].descriptors['roi'] for i in range(len(dataset_merge))]))\n",
    "        uniq_sess = sorted(list(set([dataset_merge[i].descriptors['session'] for i in range(len(dataset_merge))])))\n",
    "        for curr_roi in uniq_roi: \n",
    "            # for each run and session\n",
    "            idx = 0\n",
    "            data_across_sess = []\n",
    "            \n",
    "            idx_cond = 0\n",
    "            idx_run = 0\n",
    "            for idx_sess,curr_sess in enumerate(uniq_sess):\n",
    "                data_temp = copy.deepcopy([x for x in dataset_merge if (x.descriptors['session'] == curr_sess) and\n",
    "                                                 (x.descriptors['roi'] == curr_roi)])\n",
    "                #print(len(data_temp))\n",
    "                for rr in range(len(data_temp)):\n",
    "                    curr_ecc = data_temp[rr].descriptors['ecc_or_set']\n",
    "                    \n",
    "                    #print(curr_ecc)\n",
    "                    if (curr_ecc == 'E1') | (curr_ecc == 'P1'): # 3 dva or spatial set 1\n",
    "                        continue\n",
    "                    \n",
    "                    curr_run = data_temp[rr].descriptors['run']\n",
    "                    curr_conds = data_temp[rr].obs_descriptors['conds']\n",
    "                    curr_conds_index = data_temp[rr].obs_descriptors['conds_index']\n",
    "                    curr_run_name = data_temp[rr].obs_descriptors['run_name']\n",
    "                    \n",
    "                    curr_data = data_temp[rr].get_measurements()\n",
    "                    #print(curr_run,curr_ecc,curr_data.shape)\n",
    "\n",
    "                    # remove nan conditions\n",
    "                    # which rows (conditions) do not have all nan values (valid conditions)\n",
    "                    cond_not_nan = ~np.isnan(curr_data).all(axis=1)\n",
    "                    #print(cond_not_nan)\n",
    "\n",
    "                    # let's remove those conditions\n",
    "                    curr_data = curr_data[cond_not_nan,:]\n",
    "                    #print(curr_data.shape)\n",
    "\n",
    "                    # remove those invalid conditions from the conds and conds_index too\n",
    "                    curr_conds = [curr_conds[i] for i in range(len(cond_not_nan)) if cond_not_nan[i]]\n",
    "                    curr_conds_index = [curr_conds_index[i] for i in range(len(cond_not_nan)) if cond_not_nan[i]]\n",
    "                    #print(curr_conds,curr_conds_index)\n",
    "                    new_run_name = [curr_run_name[i] for i in range(len(cond_not_nan)) if cond_not_nan[i]]\n",
    "                    \n",
    "                    # update conditions and condition index\n",
    "                    # for the LR model\n",
    "                    # calculate new condition names and indices\n",
    "                    new_conds = [x[0]+'-'+curr_run+'-'+curr_sess for x in curr_conds]\n",
    "                        \n",
    "                    new_conds_index = [idx if x == 'C'+'-'+curr_run+'-'+curr_sess \n",
    "                                       else idx+1 for x in new_conds]\n",
    "                    idx = idx+2 #uniq_sess.index(curr_sess)*2\n",
    "                    #print(new_conds)\n",
    "                    \n",
    "                    pattern_index = [idx_cond if x == 'C'+'-'+curr_run+'-'+curr_sess \n",
    "                                     else idx_cond+1 for x in new_conds]       \n",
    "                    run_pair_index = [idx_run for _ in range(len(curr_conds))]\n",
    "                    sess_index = [idx_sess for _ in range(len(curr_conds))]\n",
    "                    idx_run += 1\n",
    "\n",
    "                    # update\n",
    "                    data_temp[rr].obs_descriptors['full_conds'] = [c[:3] for c in curr_conds]\n",
    "                    data_temp[rr].measurements = curr_data\n",
    "                    data_temp[rr].obs_descriptors['conds'] = new_conds\n",
    "                    data_temp[rr].obs_descriptors['conds_index'] = new_conds_index\n",
    "                    data_temp[rr].obs_descriptors['run_name'] = new_run_name\n",
    "                    data_temp[rr].obs_descriptors['pattern_index'] = pattern_index\n",
    "                    data_temp[rr].obs_descriptors['run_pair_index'] = run_pair_index\n",
    "                    data_temp[rr].obs_descriptors['sess_index'] = sess_index\n",
    "                    \n",
    "                    # update the descriptors too\n",
    "                    data_temp[rr].descriptors.pop('session', None)\n",
    "                    data_temp[rr].descriptors.pop('run', None)\n",
    "                    #data_temp[rr].descriptors.pop('ecc', None)\n",
    "                    data_temp[rr].descriptors['name'] = subject + ' | ' + curr_roi + ' | ' + curr_ecc\n",
    "                    \n",
    "                    \n",
    "                    #print(data_temp[rr])\n",
    "                    data_across_sess.extend([data_temp[rr]])\n",
    "                    #print(len(data_across_sess)) # 5 or 6 run combinations \n",
    "\n",
    "            #    now we can merge datasets across sessions\n",
    "            dataset_test = rsatoolbox.data.dataset.merge_subsets(data_across_sess)\n",
    "            #             print(len(dataset_test.obs_descriptors['conds']))\n",
    "            #             print(dataset_test.obs_descriptors['conds'])\n",
    "\n",
    "            # remove voxels that are nan\n",
    "            measure = dataset_test.get_measurements()\n",
    "            vox_not_nan = ~np.isnan(measure).any(axis=0) # voxels\n",
    "            measure = measure[:,vox_not_nan]\n",
    "            nVox_not_nan = measure.shape[1] # number of voxels  \n",
    "\n",
    "            # update channel descriptor and measurements\n",
    "            chn_des = {'voxels': np.array(['voxel_' + str(x) for x in np.arange(nVox_not_nan)])} # descriptors for channels\n",
    "            dataset_test.measurements = measure\n",
    "            dataset_test.channel_descriptors = chn_des\n",
    "\n",
    "            dataset_merge_new.append(dataset_test)  \n",
    "\n",
    "        # save dataset\n",
    "        with open(os.path.join(subject,subject+'_'+epochs[epoch]+'_dataset_merge_across_sess_CN_SP2.pkg'),'wb') as f:\n",
    "            pickle.dump(dataset_merge_new,f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "93894f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rsatoolbox.data.Dataset\n",
      "measurements = \n",
      "[[-4.32295465 -6.16556215  0.7349382  ... -1.17331648  0.45550582\n",
      "  -2.78499293]\n",
      " [-1.72085452 -3.63415432 -1.65540874 ...  0.33521384 -0.54659218\n",
      "  -0.26850107]\n",
      " [-3.02165127 -6.29844046 -1.63729489 ...  0.08400786  1.30360365\n",
      "  -1.44681382]\n",
      " [-3.38832998 -6.20992613  0.13150956 ...  0.90429479 -2.20506287\n",
      "  -4.67829561]\n",
      " [-3.95717478 -6.65573072 -0.20510076 ... -3.87506557 -1.18707049\n",
      "  -2.55028176]]\n",
      "...\n",
      "\n",
      "descriptors: \n",
      "ecc_or_set = P2\n",
      "roi = area4-ju50\n",
      "subj = f19\n",
      "name = f19 | area4-ju50 | P2\n",
      "\n",
      "\n",
      "obs_descriptors: \n",
      "pattern_index = [0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1\n",
      " 1 1 1 0 0 0 0 1 1 1 1]\n",
      "full_conds = ['CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR'\n",
      " 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR'\n",
      " 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR'\n",
      " 'NWL' 'NTL' 'NWR' 'NTR' 'CWL' 'CTL' 'CWR' 'CTR' 'NWL' 'NTL' 'NWR' 'NTR']\n",
      "sess_index = [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1]\n",
      "run_name = ['R2' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2' 'R3' 'R3' 'R3' 'R3' 'R3' 'R3'\n",
      " 'R3' 'R3' 'R6' 'R6' 'R6' 'R6' 'R6' 'R6' 'R6' 'R6' 'R1' 'R1' 'R1' 'R1'\n",
      " 'R1' 'R1' 'R1' 'R1' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2' 'R2' 'R4' 'R4'\n",
      " 'R4' 'R4' 'R4' 'R4' 'R4' 'R4']\n",
      "conds_index = [ 0  0  0  0  1  1  1  1  2  2  2  2  3  3  3  3  4  4  4  4  5  5  5  5\n",
      "  6  6  6  6  7  7  7  7  8  8  8  8  9  9  9  9 10 10 10 10 11 11 11 11]\n",
      "run_pair_index = [0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 4 4 4 4 4\n",
      " 4 4 4 5 5 5 5 5 5 5 5]\n",
      "conds = ['C-R2-S5934' 'C-R2-S5934' 'C-R2-S5934' 'C-R2-S5934' 'N-R2-S5934'\n",
      " 'N-R2-S5934' 'N-R2-S5934' 'N-R2-S5934' 'C-R3-S5934' 'C-R3-S5934'\n",
      " 'C-R3-S5934' 'C-R3-S5934' 'N-R3-S5934' 'N-R3-S5934' 'N-R3-S5934'\n",
      " 'N-R3-S5934' 'C-R6-S5934' 'C-R6-S5934' 'C-R6-S5934' 'C-R6-S5934'\n",
      " 'N-R6-S5934' 'N-R6-S5934' 'N-R6-S5934' 'N-R6-S5934' 'C-R1-S5946'\n",
      " 'C-R1-S5946' 'C-R1-S5946' 'C-R1-S5946' 'N-R1-S5946' 'N-R1-S5946'\n",
      " 'N-R1-S5946' 'N-R1-S5946' 'C-R2-S5946' 'C-R2-S5946' 'C-R2-S5946'\n",
      " 'C-R2-S5946' 'N-R2-S5946' 'N-R2-S5946' 'N-R2-S5946' 'N-R2-S5946'\n",
      " 'C-R4-S5946' 'C-R4-S5946' 'C-R4-S5946' 'C-R4-S5946' 'N-R4-S5946'\n",
      " 'N-R4-S5946' 'N-R4-S5946' 'N-R4-S5946']\n",
      "\n",
      "\n",
      "channel_descriptors: \n",
      "voxels = ['voxel_0' 'voxel_1' 'voxel_2' ... 'voxel_1354' 'voxel_1355' 'voxel_1356']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset_merge_new[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c219609c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ea7c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4093cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b730ec27",
   "metadata": {},
   "source": [
    "# The END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5195989f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921b070e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
